{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b87fe9b-9b1b-4460-9f46-e640f6aaaa9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 509ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import threading\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "import time\n",
    "import torch\n",
    "import pyaudio\n",
    "import wave\n",
    "import speech_recognition as sr\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from transformers import ElectraForSequenceClassification, AutoTokenizer\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# 비디오 모델 로드\n",
    "video_model = load_model('MPANET.h5')\n",
    "\n",
    "# 텍스트 모델 및 토크나이저 로드\n",
    "num_classes = 4\n",
    "text_model = ElectraForSequenceClassification.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", num_labels=num_classes)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "text_model.load_state_dict(torch.load(\"model.pt\"))\n",
    "\n",
    "# 감정 레이블\n",
    "emotion_labels = ['anger', 'happiness', 'panic', 'sadness']\n",
    "\n",
    "# 점수 초기화\n",
    "emotion_score, text_score, keyword_score, total_score = 10, 0, 10, 0\n",
    "\n",
    "# 웹캠 및 마이크 설정\n",
    "recording, frames = False, []\n",
    "\n",
    "# PyAudio 설정\n",
    "FORMAT, CHANNELS, RATE, CHUNK = pyaudio.paInt16, 1, 44100, 2048\n",
    "WAVE_OUTPUT_FILENAME = \"output.wav\"\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "# CSV 파일 읽기 및 질문/키워드 설정\n",
    "df = pd.read_csv('Software Questions.csv')\n",
    "questions, keywords_list = df['question'].tolist(), df['keyword'].tolist()\n",
    "random_question, random_keywords = \"\", []\n",
    "\n",
    "def select_random_question():\n",
    "    global random_question, random_keywords\n",
    "    random_index = random.randint(0, len(questions) - 1)\n",
    "    random_question, random_keywords = questions[random_index], keywords_list[random_index].split('/')\n",
    "\n",
    "# 초기 질문과 키워드 선택\n",
    "select_random_question()\n",
    "\n",
    "def record_video_and_audio():\n",
    "    global frames, recording\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while not cap.isOpened():\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "    messagebox.showinfo(\"Info\", \"로딩이 완료 되었습니다. 답변을 진행해주세요.\")\n",
    "    out = cv2.VideoWriter('output.avi', cv2.VideoWriter_fourcc(*'XVID'), 20.0, (640, 480))\n",
    "    stream = audio.open(format=FORMAT, channels=CHANNELS, rate=RATE, input=True, frames_per_buffer=CHUNK)\n",
    "\n",
    "    while recording:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            out.write(frame)\n",
    "            cv2.imshow('Recording', frame)\n",
    "            frames.append(stream.read(CHUNK))\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    cv2.destroyAllWindows()\n",
    "    save_audio()\n",
    "    analyze_video_and_audio()\n",
    "\n",
    "def save_audio():\n",
    "    with wave.open(WAVE_OUTPUT_FILENAME, 'wb') as wf:\n",
    "        wf.setnchannels(CHANNELS)\n",
    "        wf.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "        wf.setframerate(RATE)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "def predict_emotion_video(frame):\n",
    "    predictions = video_model.predict(np.reshape(cv2.resize(frame, (224, 224)) / 255.0, (1, 224, 224, 3)))\n",
    "    return predictions\n",
    "\n",
    "def analyze_video():\n",
    "    global emotion_score\n",
    "    cap = cv2.VideoCapture('output.avi')\n",
    "    frame_interval, frame_count = int(cap.get(cv2.CAP_PROP_FPS) * 0.5), 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_count % frame_interval == 0:\n",
    "            predictions = predict_emotion_video(frame)\n",
    "            max_index, confidence = np.argmax(predictions), predictions[0][np.argmax(predictions)]\n",
    "            if confidence >= 0.75:\n",
    "                update_emotion_score(emotion_labels[max_index])\n",
    "        frame_count += 1\n",
    "    cap.release()\n",
    "    update_video_counts()\n",
    "    update_score()\n",
    "\n",
    "def update_emotion_score(emotion):\n",
    "    global emotion_score, video_emotion_counts\n",
    "    video_emotion_counts[emotion] += 1\n",
    "    emotion_score += 1 if emotion == 'happiness' else -0.5\n",
    "\n",
    "def analyze_audio():\n",
    "    recognizer = sr.Recognizer()\n",
    "    global text_score, keyword_score\n",
    "    with sr.AudioFile(WAVE_OUTPUT_FILENAME) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        try:\n",
    "            text = recognizer.recognize_google(audio_data, language='ko-KR')\n",
    "            emotion = predict_text_emotion(text)\n",
    "            keyword_counts = count_keywords(text, random_keywords)\n",
    "            update_text_results(text, emotion, keyword_counts)\n",
    "\n",
    "            # Calculate text score based on emotion\n",
    "            text_score = {'happiness': 10, 'panic': 5, 'sadness': 5, 'anger': 0}.get(emotion, 0)\n",
    "\n",
    "            # Calculate keyword score\n",
    "            missed_keywords = len([k for k in random_keywords if keyword_counts[k] == 0])\n",
    "            if len(random_keywords) > 0:\n",
    "                penalty_per_keyword = 10 / len(random_keywords)\n",
    "                keyword_score = max(0, 10 - missed_keywords * penalty_per_keyword)\n",
    "            else:\n",
    "                keyword_score = 10\n",
    "\n",
    "            update_score()\n",
    "        except (sr.UnknownValueError, sr.RequestError) as e:\n",
    "            messagebox.showerror(\"Error\", f\"Could not process audio: {e}\")\n",
    "\n",
    "\n",
    "def analyze_video_and_audio():\n",
    "    threading.Thread(target=analyze_video).start()\n",
    "    threading.Thread(target=analyze_audio).start()\n",
    "\n",
    "def count_keywords(text, keywords):\n",
    "    return {keyword: len(re.findall(re.escape(keyword), text.lower())) for keyword in keywords}\n",
    "\n",
    "def update_video_counts():\n",
    "    for label in emotion_labels:\n",
    "        video_count_labels[label].config(text=f'{label}: {video_emotion_counts[label]}')\n",
    "\n",
    "def update_text_results(text, emotion, keyword_counts):\n",
    "    result_text = f\"Text: {text}\\nEmotion: {emotion}\\n\\nKeyword Counts:\\n\" + \"\\n\".join(f\"{keyword}: {count}\" for keyword, count in keyword_counts.items())\n",
    "    text_result_label.config(text=result_text)\n",
    "\n",
    "def update_score():\n",
    "    global total_score\n",
    "    total_score = emotion_score + text_score + keyword_score\n",
    "    score_label.config(text=f\"Emotion Score: {emotion_score:.1f}\\nText Score: {text_score:.1f}\\nKeyword Score: {keyword_score:.1f}\\nTotal Score: {total_score:.1f}\")\n",
    "\n",
    "\n",
    "def predict_text_emotion(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    outputs = text_model(**inputs)\n",
    "    return emotion_labels[torch.argmax(torch.nn.functional.softmax(outputs.logits, dim=-1))]\n",
    "\n",
    "def start_recording():\n",
    "    global recording, frames, emotion_score, text_score, keyword_score, video_emotion_counts\n",
    "    recording, frames, emotion_score, text_score, keyword_score = True, [], 10, 0, 10\n",
    "    video_emotion_counts = {label: 0 for label in emotion_labels}\n",
    "    threading.Thread(target=record_video_and_audio).start()\n",
    "\n",
    "def stop_recording():\n",
    "    global recording\n",
    "    recording = False\n",
    "\n",
    "def change_question():\n",
    "    select_random_question()\n",
    "    question_label.config(text=f\"Q: {random_question}\")\n",
    "\n",
    "def create_label(frame, text, font, fg, bg):\n",
    "    label = tk.Label(frame, text=text, font=font, fg=fg, bg=bg)\n",
    "    label.pack(pady=10)\n",
    "    return label\n",
    "\n",
    "def create_button(frame, text, command, font, fg, bg):\n",
    "    button = tk.Button(frame, text=text, command=command, font=font, fg=fg, bg=bg)\n",
    "    button.pack(pady=10)\n",
    "    return button\n",
    "\n",
    "def start_gui():\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Interview Practice\")\n",
    "    root.geometry(\"800x1050\")\n",
    "    root.resizable(False, False)\n",
    "\n",
    "    global question_label, score_label, text_result_label, video_count_labels\n",
    "    chalk_font, fg_color, bg_color = (\"Comic Sans MS\", 12, \"bold\"), \"#000000\", \"#FFFFFF\"\n",
    "    root.configure(bg=bg_color)\n",
    "\n",
    "    def create_fixed_frame(root, width, height):\n",
    "        frame = tk.Frame(root, padx=10, pady=10, bd=2, relief=tk.GROOVE, bg=bg_color, width=width, height=height)\n",
    "        frame.pack_propagate(False)\n",
    "        frame.pack(pady=10)\n",
    "        return frame\n",
    "\n",
    "    question_frame = create_fixed_frame(root, 760, 140)\n",
    "    question_label = create_label(question_frame, f\"Q: {random_question}\", chalk_font, fg_color, bg_color)\n",
    "    change_question_button = create_button(question_frame, \"질문 변경\", change_question, chalk_font, fg_color, bg_color)\n",
    "\n",
    "    button_frame = create_fixed_frame(root, 760, 140)\n",
    "    start_button = create_button(button_frame, \"답변 시작\", start_recording, chalk_font, fg_color, bg_color)\n",
    "    stop_button = create_button(button_frame, \"답변 종료\", stop_recording, chalk_font, fg_color, bg_color)\n",
    "\n",
    "    result_frame = create_fixed_frame(root, 760, 700)\n",
    "    create_label(result_frame, \"얼굴 감정 분석:\", chalk_font, fg_color, bg_color)\n",
    "    video_count_labels = {label: create_label(result_frame, f'{label}: 0', chalk_font, fg_color, bg_color) for label in emotion_labels}\n",
    "\n",
    "    create_label(result_frame, \"음성 감정 분석:\", chalk_font, fg_color, bg_color)\n",
    "    text_result_label = create_label(result_frame, \"\", chalk_font, fg_color, bg_color)\n",
    "    text_result_label.config(wraplength=500, justify=\"left\")\n",
    "\n",
    "    score_label = create_label(result_frame, \"Emotion Score: 0\\nText Score: 0\\nKeyword Score: 0\\nTotal Score: 0\", chalk_font, fg_color, bg_color)\n",
    "\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_gui()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1cf8d4-71cf-4b8c-a434-72910e10ad43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
