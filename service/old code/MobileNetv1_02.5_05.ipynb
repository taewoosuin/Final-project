{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17532,"status":"ok","timestamp":1716587735780,"user":{"displayName":"박태우","userId":"12492517778931241346"},"user_tz":-540},"id":"xV7z-yIHva2K","outputId":"084e8225-9b69-429a-a218-91e1f5e07d05"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"ECxNcYjjUaKH"},"source":["#데이터 전처리"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3078,"status":"ok","timestamp":1716587738854,"user":{"displayName":"박태우","userId":"12492517778931241346"},"user_tz":-540},"id":"4nI_ceMCZw0N"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dropout, Dense, Reshape, SeparableConv2D, Conv2D, BatchNormalization, Multiply, Layer,Attention, LayerNormalization, Add\n","from tensorflow.keras.applications import MobileNet\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12633,"status":"ok","timestamp":1716587751484,"user":{"displayName":"박태우","userId":"12492517778931241346"},"user_tz":-540},"id":"0C-5LhNHJapI","outputId":"9e4a9ddd-2cf3-4bfe-877a-d6aba149a5ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 5614 images belonging to 4 classes.\n","Found 1131 images belonging to 4 classes.\n","Found 1137 images belonging to 4 classes.\n"]}],"source":["# 데이터 경로\n","train_data_dir = '/content/drive/MyDrive/Data/img_best/train'\n","validation_data_dir = '/content/drive/MyDrive/Data/img_best/val'\n","test_data_dir = '/content/drive/MyDrive/Data/img/test'\n","\n","# 데이터 증강\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=10,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest',\n","    brightness_range=[0.8, 1.2]\n",")\n","\n","val_test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# 이미지 불러오기\n","train_generator = train_datagen.flow_from_directory(\n","    train_data_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical'\n",")\n","\n","validation_generator = val_test_datagen.flow_from_directory(\n","    validation_data_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical'\n",")\n","\n","test_generator = val_test_datagen.flow_from_directory(\n","    test_data_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical'\n",")"]},{"cell_type":"markdown","metadata":{"id":"-AyS03TtfQzB"},"source":["#MobileNetV1 기반 모델링(실행환경 T4)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1716587751484,"user":{"displayName":"박태우","userId":"12492517778931241346"},"user_tz":-540},"id":"HBTsW46RIkF5"},"outputs":[],"source":["# Base model 정의\n","def create_base_model(input_shape):\n","    # Load the base MobileNet model with pre-trained ImageNet weights\n","    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=input_shape)\n","    base_model.trainable = False\n","\n","    # Get the output from the fourth last layer\n","    layer_index = -14\n","    output_layer = base_model.layers[layer_index].output\n","\n","    # Create the new model\n","    base_model = Model(inputs=base_model.input, outputs=output_layer)\n","\n","    return base_model\n","\n","\n","# 패치 추출 레이어 정의\n","def patch_extraction_layer():\n","    return tf.keras.Sequential([\n","        SeparableConv2D(256, kernel_size=4, strides=4, padding='same', activation='relu'),\n","        SeparableConv2D(256, kernel_size=2, strides=2, padding='valid', activation='relu'),\n","        Conv2D(256, kernel_size=1, strides=1, padding='valid', activation='relu')\n","    ], name='patch_extraction')\n","\n","# Pre-classification 레이어 정의\n","def create_pre_classification_layer():\n","    return tf.keras.Sequential([\n","        Dense(32, activation='relu'),\n","        BatchNormalization()\n","    ], name='pre_classification')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2218,"status":"ok","timestamp":1716587753699,"user":{"displayName":"박태우","userId":"12492517778931241346"},"user_tz":-540},"id":"Ba41_ozA4V70","outputId":"46937327-301d-4540-f607-b886a52e7477"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n","17225924/17225924 [==============================] - 0s 0us/step\n","Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_2 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n","                                                                                                  \n"," model (Functional)          (None, 14, 14, 512)          1627840   ['input_2[0][0]']             \n","                                                                                                  \n"," patch_extraction (Sequenti  (None, 2, 2, 256)            272128    ['model[0][0]']               \n"," al)                                                                                              \n","                                                                                                  \n"," gap (GlobalAveragePooling2  (None, 256)                  0         ['patch_extraction[0][0]']    \n"," D)                                                                                               \n","                                                                                                  \n"," dropout (Dropout)           (None, 256)                  0         ['gap[0][0]']                 \n","                                                                                                  \n"," pre_classification (Sequen  (None, 32)                   8352      ['dropout[0][0]']             \n"," tial)                                                                                            \n","                                                                                                  \n"," attention (Attention)       (None, 32)                   1         ['pre_classification[0][0]',  \n","                                                                     'pre_classification[0][0]']  \n","                                                                                                  \n"," add (Add)                   (None, 32)                   0         ['pre_classification[0][0]',  \n","                                                                     'attention[0][0]']           \n","                                                                                                  \n"," dense_1 (Dense)             (None, 4)                    132       ['add[0][0]']                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 1908453 (7.28 MB)\n","Trainable params: 280549 (1.07 MB)\n","Non-trainable params: 1627904 (6.21 MB)\n","__________________________________________________________________________________________________\n"]}],"source":["def create_mobilenet_with_cbam_block(input_shape, num_classes):\n","    base_model = create_base_model(input_shape)\n","    inputs = Input(shape=input_shape)\n","    # 기본 모델\n","    x = base_model(inputs)\n","    # 패치 추출 레이어\n","    x = patch_extraction_layer()(x)\n","    # GlobalAveragePooling2D 및 Dropout\n","    x = GlobalAveragePooling2D(name='gap')(x)\n","    x = Dropout(0.3)(x)\n","    # 사전 분류 레이어\n","    x = create_pre_classification_layer()(x)\n","    # Self-Attention 추가\n","    attn_output = Attention(use_scale=True)([x, x])\n","    x = Add()([x, attn_output])\n","    # 출력층\n","    outputs = Dense(num_classes, activation='softmax')(x)\n","    model = Model(inputs, outputs)\n","    return model\n","\n","\n","# 모델 생성 및 컴파일\n","model = create_mobilenet_with_cbam_block(input_shape=(224, 224, 3), num_classes=4)\n","model.compile(optimizer=Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# 모델 요약 출력\n","model.summary()"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1716587753699,"user":{"displayName":"박태우","userId":"12492517778931241346"},"user_tz":-540},"id":"qQdn9ZuV0CeI"},"outputs":[],"source":["# 학습률 조정 콜백\n","reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=2, min_delta=0.005, min_lr=1e-7)\n","\n","# EarlyStopping 콜백\n","early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, min_delta=0.005, restore_best_weights=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"F9Qda9o00Ex4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","176/176 [==============================] - 3040s 17s/step - loss: 1.3897 - accuracy: 0.2636 - val_loss: 1.3902 - val_accuracy: 0.2511 - lr: 0.0010\n","Epoch 2/100\n","176/176 [==============================] - 403s 2s/step - loss: 1.3727 - accuracy: 0.2937 - val_loss: 1.3946 - val_accuracy: 0.2493 - lr: 0.0010\n","Epoch 3/100\n","176/176 [==============================] - 401s 2s/step - loss: 1.2944 - accuracy: 0.3881 - val_loss: 1.3571 - val_accuracy: 0.3183 - lr: 0.0010\n","Epoch 4/100\n","176/176 [==============================] - 406s 2s/step - loss: 1.1905 - accuracy: 0.4626 - val_loss: 1.2662 - val_accuracy: 0.4156 - lr: 0.0010\n","Epoch 5/100\n","176/176 [==============================] - 405s 2s/step - loss: 1.1019 - accuracy: 0.5191 - val_loss: 1.1180 - val_accuracy: 0.5137 - lr: 0.0010\n","Epoch 6/100\n","176/176 [==============================] - 420s 2s/step - loss: 1.0449 - accuracy: 0.5527 - val_loss: 1.1245 - val_accuracy: 0.5314 - lr: 0.0010\n","Epoch 7/100\n","176/176 [==============================] - 399s 2s/step - loss: 1.0106 - accuracy: 0.5768 - val_loss: 1.1873 - val_accuracy: 0.5279 - lr: 0.0010\n","Epoch 8/100\n","176/176 [==============================] - 404s 2s/step - loss: 0.9746 - accuracy: 0.5898 - val_loss: 1.1969 - val_accuracy: 0.5340 - lr: 0.0010\n","Epoch 9/100\n","176/176 [==============================] - 405s 2s/step - loss: 0.9340 - accuracy: 0.6120 - val_loss: 0.9564 - val_accuracy: 0.5897 - lr: 1.0000e-04\n","Epoch 10/100\n","176/176 [==============================] - 407s 2s/step - loss: 0.9214 - accuracy: 0.6099 - val_loss: 0.9735 - val_accuracy: 0.5933 - lr: 1.0000e-04\n","Epoch 11/100\n","176/176 [==============================] - 400s 2s/step - loss: 0.9246 - accuracy: 0.6104 - val_loss: 0.9752 - val_accuracy: 0.5933 - lr: 1.0000e-04\n","Epoch 12/100\n","176/176 [==============================] - 408s 2s/step - loss: 0.9140 - accuracy: 0.6160 - val_loss: 0.9399 - val_accuracy: 0.6065 - lr: 1.0000e-05\n","Epoch 13/100\n","176/176 [==============================] - 414s 2s/step - loss: 0.9123 - accuracy: 0.6199 - val_loss: 0.9440 - val_accuracy: 0.5924 - lr: 1.0000e-05\n","Epoch 14/100\n","176/176 [==============================] - 413s 2s/step - loss: 0.9088 - accuracy: 0.6206 - val_loss: 0.9436 - val_accuracy: 0.6030 - lr: 1.0000e-05\n","Epoch 15/100\n","176/176 [==============================] - 418s 2s/step - loss: 0.9281 - accuracy: 0.6038 - val_loss: 0.9459 - val_accuracy: 0.6004 - lr: 1.0000e-06\n","Epoch 16/100\n","176/176 [==============================] - 410s 2s/step - loss: 0.9105 - accuracy: 0.6208 - val_loss: 0.9448 - val_accuracy: 0.5942 - lr: 1.0000e-06\n","Epoch 17/100\n","176/176 [==============================] - 408s 2s/step - loss: 0.9174 - accuracy: 0.6135 - val_loss: 0.9421 - val_accuracy: 0.5977 - lr: 1.0000e-07\n"]}],"source":["# 모델 훈련\n","history = model.fit(\n","    train_generator,\n","    epochs=100,\n","    validation_data=validation_generator,\n","    callbacks=[reduce_lr, early_stopping]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IYiJIHEuMmcM"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_2 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n","                                                                                                  \n"," model (Functional)          (None, 14, 14, 512)          1627840   ['input_2[0][0]']             \n","                                                                                                  \n"," patch_extraction (Sequenti  (None, None, None, 256)      272128    ['model[0][0]']               \n"," al)                                                                                              \n","                                                                                                  \n"," gap (GlobalAveragePooling2  (None, 256)                  0         ['patch_extraction[0][0]']    \n"," D)                                                                                               \n","                                                                                                  \n"," dropout (Dropout)           (None, 256)                  0         ['gap[0][0]']                 \n","                                                                                                  \n"," pre_classification (Sequen  (None, 32)                   8352      ['dropout[0][0]']             \n"," tial)                                                                                            \n","                                                                                                  \n"," attention (Attention)       (None, 32)                   1         ['pre_classification[0][0]',  \n","                                                                     'pre_classification[0][0]']  \n","                                                                                                  \n"," add (Add)                   (None, 32)                   0         ['pre_classification[0][0]',  \n","                                                                     'attention[0][0]']           \n","                                                                                                  \n"," dense_1 (Dense)             (None, 4)                    132       ['add[0][0]']                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 1908453 (7.28 MB)\n","Trainable params: 1614309 (6.16 MB)\n","Non-trainable params: 294144 (1.12 MB)\n","__________________________________________________________________________________________________\n"]}],"source":["# 파인튜닝 과정\n","base_model = model.layers[1]\n","base_model.trainable = True\n","\n","num_freeze_layers = 43\n","\n","# 하위 레이어 고정, 상위 레이어 해제\n","for layer in base_model.layers[:num_freeze_layers]:\n","    layer.trainable = False\n","for layer in base_model.layers[num_freeze_layers:]:\n","    if isinstance(layer, BatchNormalization):\n","        layer.trainable = False\n","    else:\n","        layer.trainable = True\n","\n","# 모델 재컴파일\n","model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# 모델 요약 출력\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qG5UtjAI11_K"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","176/176 [==============================] - 418s 2s/step - loss: 0.9229 - accuracy: 0.6094 - val_loss: 1.2349 - val_accuracy: 0.5703 - lr: 1.0000e-04\n","Epoch 2/100\n","176/176 [==============================] - 409s 2s/step - loss: 0.8021 - accuracy: 0.6712 - val_loss: 0.8173 - val_accuracy: 0.6534 - lr: 1.0000e-04\n","Epoch 3/100\n","176/176 [==============================] - 407s 2s/step - loss: 0.7408 - accuracy: 0.7057 - val_loss: 0.8268 - val_accuracy: 0.6569 - lr: 1.0000e-04\n","Epoch 4/100\n","176/176 [==============================] - 411s 2s/step - loss: 0.7051 - accuracy: 0.7157 - val_loss: 0.7298 - val_accuracy: 0.6923 - lr: 1.0000e-04\n","Epoch 5/100\n","176/176 [==============================] - 427s 2s/step - loss: 0.6676 - accuracy: 0.7351 - val_loss: 0.9287 - val_accuracy: 0.6401 - lr: 1.0000e-04\n","Epoch 6/100\n","176/176 [==============================] - 407s 2s/step - loss: 0.6416 - accuracy: 0.7472 - val_loss: 2.3039 - val_accuracy: 0.4103 - lr: 1.0000e-04\n","Epoch 7/100\n","176/176 [==============================] - 406s 2s/step - loss: 0.5806 - accuracy: 0.7779 - val_loss: 0.7116 - val_accuracy: 0.7011 - lr: 1.0000e-05\n","Epoch 8/100\n","176/176 [==============================] - 404s 2s/step - loss: 0.5544 - accuracy: 0.7873 - val_loss: 0.7044 - val_accuracy: 0.7135 - lr: 1.0000e-05\n","Epoch 9/100\n","176/176 [==============================] - 416s 2s/step - loss: 0.5489 - accuracy: 0.7939 - val_loss: 0.6752 - val_accuracy: 0.7224 - lr: 1.0000e-05\n","Epoch 10/100\n","176/176 [==============================] - 410s 2s/step - loss: 0.5440 - accuracy: 0.7918 - val_loss: 0.6991 - val_accuracy: 0.7259 - lr: 1.0000e-05\n","Epoch 11/100\n","176/176 [==============================] - 408s 2s/step - loss: 0.5241 - accuracy: 0.8023 - val_loss: 0.7395 - val_accuracy: 0.7029 - lr: 1.0000e-05\n","Epoch 12/100\n","176/176 [==============================] - 405s 2s/step - loss: 0.5245 - accuracy: 0.8039 - val_loss: 0.6799 - val_accuracy: 0.7224 - lr: 1.0000e-06\n","Epoch 13/100\n","176/176 [==============================] - 410s 2s/step - loss: 0.5091 - accuracy: 0.8103 - val_loss: 0.6777 - val_accuracy: 0.7162 - lr: 1.0000e-06\n","Epoch 14/100\n","176/176 [==============================] - 416s 2s/step - loss: 0.5247 - accuracy: 0.8048 - val_loss: 0.6803 - val_accuracy: 0.7144 - lr: 1.0000e-07\n"]}],"source":["# 모델 훈련\n","history = model.fit(\n","    train_generator,\n","    epochs=100,\n","    validation_data=validation_generator,\n","    callbacks=[reduce_lr, early_stopping]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ljnV7EoWv8Ak"},"outputs":[{"name":"stdout","output_type":"stream","text":["36/36 [==============================] - 60s 2s/step - loss: 0.6671 - accuracy: 0.7233\n","Validation Loss: 0.6670974493026733\n","Validation Accuracy: 0.7232537865638733\n"]}],"source":["# 검증 데이터 확인\n","loss, accuracy = model.evaluate(validation_generator)\n","print(f\"Validation Loss: {loss}\")\n","print(f\"Validation Accuracy: {accuracy}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"collapsed":true,"id":"gu7AzcLndjh3"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}],"source":["# 모델 저장\n","model.save('/content/drive/MyDrive/MobileNetV1_2.5-05.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3635695,"status":"ok","timestamp":1716538347505,"user":{"displayName":"박태우","userId":"12492517778931241346"},"user_tz":-540},"id":"3pj5Tf_G0IkG","outputId":"abb2c506-e92c-45cd-e439-b51daf7c359a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 15392 images belonging to 4 classes.\n","481/481 [==============================] - 9170s 19s/step - loss: 1.3011 - accuracy: 0.3961\n","Test Loss: 1.3011044263839722\n","Test Accuracy: 0.39611485600471497\n","481/481 [==============================] - 44s 90ms/step\n","Accuracy for class anger: 66.50%\n","Accuracy for class happy: 40.98%\n","Accuracy for class panic: 10.47%\n","Accuracy for class sadness: 47.46%\n","\n","Misclassified Cases Summary:\n","   True Label Predicted Label  Count\n","0       anger           happy      5\n","1       anger           panic      2\n","2       anger         sadness   1071\n","3       happy           anger   1661\n","4       happy           panic      1\n","5       happy         sadness   1315\n","6       panic           anger   3005\n","7       panic           happy     64\n","8       panic         sadness    547\n","9     sadness           anger   1601\n","10    sadness           happy     10\n","11    sadness           panic     13\n"]}],"source":["from tensorflow.keras.models import load_model\n","from sklearn.metrics import accuracy_score\n","import pandas as pd\n","\n","# 모델 파일 경로\n","model_path = '/content/drive/MyDrive/MobileNetV1_2.5-05.h5'\n","\n","# 모델 로드\n","model = load_model(model_path)\n","\n","# 테스트 데이터 디렉토리 경로\n","test_data_dir = '/content/drive/MyDrive/Data/img/test'\n","\n","# 테스트 데이터를 위한 ImageDataGenerator 생성\n","val_test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# 테스트 데이터 생성기 생성\n","test_generator = val_test_datagen.flow_from_directory(\n","    test_data_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical',\n","    shuffle=False\n",")\n","\n","# 테스트 데이터 전체 검증\n","test_loss, test_accuracy = model.evaluate(test_generator)\n","print(f\"Test Loss: {test_loss}\")\n","print(f\"Test Accuracy: {test_accuracy}\")\n","\n","# 예측 값 얻기\n","predictions = model.predict(test_generator)\n","predicted_classes = np.argmax(predictions, axis=1)\n","true_classes = test_generator.classes\n","class_labels = list(test_generator.class_indices.keys())\n","\n","# 각 클래스별 정확도 계산 및 잘못 예측한 경우 정리\n","misclassified_summary = []\n","\n","for i, label in enumerate(class_labels):\n","    indices = np.where(true_classes == i)[0]\n","    class_accuracy = accuracy_score(true_classes[indices], predicted_classes[indices])\n","    print(f\"Accuracy for class {label}: {class_accuracy * 100:.2f}%\")\n","\n","    # 잘못 예측한 경우 저장\n","    misclassified_indices = indices[true_classes[indices] != predicted_classes[indices]]\n","    for index in misclassified_indices:\n","        true_label = class_labels[true_classes[index]]\n","        predicted_label = class_labels[predicted_classes[index]]\n","        misclassified_summary.append((true_label, predicted_label))\n","\n","# DataFrame으로 정리\n","misclassified_df = pd.DataFrame(misclassified_summary, columns=[\"True Label\", \"Predicted Label\"])\n","\n","# 잘못 예측한 경우를 집계\n","misclassified_counts = misclassified_df.groupby(['True Label', 'Predicted Label']).size().reset_index(name='Count')\n","\n","# 정리된 결과 출력\n","print(\"\\nMisclassified Cases Summary:\")\n","print(misclassified_counts)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":254716,"status":"ok","timestamp":1716528110946,"user":{"displayName":"박태우","userId":"12492517778931241346"},"user_tz":-540},"id":"dmGfxPmhBfAy","outputId":"05c411f6-1894-4aa2-f3fa-23c8b5f11f34"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 1137 images belonging to 4 classes.\n","36/36 [==============================] - 62s 2s/step - loss: 0.3849 - accuracy: 0.8522\n","36/36 [==============================] - 60s 2s/step - loss: 0.3770 - accuracy: 0.8566\n","Model 1 - Test Loss: 0.38487330079078674, Test Accuracy: 0.8522427678108215\n","Model 2 - Test Loss: 0.3770179748535156, Test Accuracy: 0.8566402792930603\n","36/36 [==============================] - 60s 2s/step\n","36/36 [==============================] - 61s 2s/step\n","Accuracy for class anger: 75.36%\n","Accuracy for class happy: 97.32%\n","Accuracy for class panic: 92.73%\n","Accuracy for class sadness: 81.34%\n","\n","Misclassified Cases Summary:\n","   True Label Predicted Label  Count\n","0       anger           happy      4\n","1       anger           panic     42\n","2       anger         sadness     23\n","3       happy           anger      5\n","4       happy           panic      2\n","5       happy         sadness      1\n","6       panic           anger     15\n","7       panic           happy      3\n","8       panic         sadness      2\n","9     sadness           anger     37\n","10    sadness           happy      5\n","11    sadness           panic     11\n"]}],"source":["#앙상블 테스트\n","from tensorflow.keras.models import load_model\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","import pandas as pd\n","\n","# 모델 파일 경로\n","model_path_1 = '/content/drive/MyDrive/model/MobileNetV1_02-13.h5'\n","model_path_2 = '/content/drive/MyDrive/MobileNetV1_02-13_relabel.h5'  # 두 번째 모델 경로\n","\n","# 모델 로드\n","model1 = load_model(model_path_1)\n","model2 = load_model(model_path_2)\n","\n","# 테스트 데이터 디렉토리 경로\n","test_data_dir = '/content/drive/MyDrive/Data/img/test'\n","\n","# 테스트 데이터를 위한 ImageDataGenerator 생성\n","val_test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# 테스트 데이터 생성기 생성\n","test_generator = val_test_datagen.flow_from_directory(\n","    test_data_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical',\n","    shuffle=False\n",")\n","\n","# 테스트 데이터 전체 검증\n","test_loss_1, test_accuracy_1 = model1.evaluate(test_generator)\n","test_loss_2, test_accuracy_2 = model2.evaluate(test_generator)\n","print(f\"Model 1 - Test Loss: {test_loss_1}, Test Accuracy: {test_accuracy_1}\")\n","print(f\"Model 2 - Test Loss: {test_loss_2}, Test Accuracy: {test_accuracy_2}\")\n","\n","# 예측 값 얻기\n","predictions1 = model1.predict(test_generator)\n","predictions2 = model2.predict(test_generator)\n","\n","# 두 모델의 예측을 평균화\n","ensemble_predictions = (predictions1 + predictions2) / 2\n","predicted_classes = np.argmax(ensemble_predictions, axis=1)\n","true_classes = test_generator.classes\n","class_labels = list(test_generator.class_indices.keys())\n","\n","# 각 클래스별 정확도 계산 및 잘못 예측한 경우 정리\n","misclassified_summary = []\n","\n","for i, label in enumerate(class_labels):\n","    indices = np.where(true_classes == i)[0]\n","    class_accuracy = accuracy_score(true_classes[indices], predicted_classes[indices])\n","    print(f\"Accuracy for class {label}: {class_accuracy * 100:.2f}%\")\n","\n","    # 잘못 예측한 경우 저장\n","    misclassified_indices = indices[true_classes[indices] != predicted_classes[indices]]\n","    for index in misclassified_indices:\n","        true_label = class_labels[true_classes[index]]\n","        predicted_label = class_labels[predicted_classes[index]]\n","        misclassified_summary.append((true_label, predicted_label))\n","\n","# DataFrame으로 정리\n","misclassified_df = pd.DataFrame(misclassified_summary, columns=[\"True Label\", \"Predicted Label\"])\n","\n","# 잘못 예측한 경우를 집계\n","misclassified_counts = misclassified_df.groupby(['True Label', 'Predicted Label']).size().reset_index(name='Count')\n","\n","# 정리된 결과 출력\n","print(\"\\nMisclassified Cases Summary:\")\n","print(misclassified_counts)\n"]},{"cell_type":"markdown","metadata":{"id":"p5kkjYL9WQnh"},"source":["#Sub"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J1E-s7AEBlW8"},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","from tensorflow.keras.preprocessing import image\n","\n","# 이미지 전처리 함수\n","def load_and_prepare_image(image_path, target_size=(224, 224)):\n","    img = image.load_img(image_path, target_size=target_size)\n","    img_tensor = image.img_to_array(img)\n","    img_tensor = np.expand_dims(img_tensor, axis=0)\n","    img_tensor /= 255.0  # 정규화\n","    return img_tensor\n","\n","# 감정을 예측하는 함수\n","def predict_emotion(model, img_path):\n","    print(\"이미지 로딩 중...\")\n","    test_image = load_and_prepare_image(img_path)\n","    print(\"감정 예측 중...\")\n","    prediction = model.predict(test_image)\n","    return prediction\n","\n","# 모델 경로와 이미지 경로\n","model_path = '/content/drive/MyDrive/MobileNetV1-6.h5'  # 학습된 모델 경로\n","image_path = '/content/drive/MyDrive/testimg/5.jpg'  # 테스트할 이미지 경로\n","\n","# 모델 로드\n","model = load_model(model_path)\n","\n","# 예측 결과 얻기\n","predictions = predict_emotion(model, image_path)\n","emotion_index = np.argmax(predictions)\n","emotion_labels = ['화남', '행복', '슬픔', '충격']  # 실제 감정 레이블로 변경\n","\n","print(\"예측된 감정:\", emotion_labels[emotion_index])\n","print(\"각 감정의 비율:\")\n","for i, label in enumerate(emotion_labels):\n","    print(f\"{label}: {predictions[0][i] * 100:.2f}%\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["ECxNcYjjUaKH"],"gpuType":"T4","machine_shape":"hm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}