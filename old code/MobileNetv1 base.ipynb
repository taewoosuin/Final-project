{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16774,"status":"ok","timestamp":1716259414638,"user":{"displayName":"박태우","userId":"12492517778931241346"},"user_tz":-540},"id":"xV7z-yIHva2K","outputId":"0faa6df7-029f-45cd-b6ee-16f3828715ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6040,"status":"ok","timestamp":1716072845562,"user":{"displayName":"이스트캠퍼스","userId":"11506478735001745454"},"user_tz":-540},"id":"JWAvAFSoVFm2","outputId":"b93f0851-70dc-4a5c-8937-bae0877ba4e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting tensorflow-addons\n","  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/611.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/611.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.0)\n","Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n","  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n","Installing collected packages: typeguard, tensorflow-addons\n","Successfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n"]}],"source":["!pip install tensorflow-addons\n"]},{"cell_type":"markdown","metadata":{"id":"ECxNcYjjUaKH"},"source":["#데이터 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4nI_ceMCZw0N"},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dropout, Dense, Reshape, MultiHeadAttention\n","from tensorflow.keras.applications import MobileNet\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","\n","# 난수 시드 고정\n","def set_seed(seed=42):\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    tf.random.set_seed(seed)\n","\n","# 시드 고정\n","set_seed(42)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14647,"status":"ok","timestamp":1716259432658,"user":{"displayName":"박태우","userId":"12492517778931241346"},"user_tz":-540},"id":"0C-5LhNHJapI","outputId":"cefcabff-2f70-4178-f4a6-ddfb2bba163d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 5853 images belonging to 4 classes.\n","Found 1168 images belonging to 4 classes.\n","Found 1137 images belonging to 4 classes.\n"]}],"source":["# 데이터 경로\n","train_data_dir = '/content/drive/MyDrive/Data/img/train'\n","validation_data_dir = '/content/drive/MyDrive/Data/img/val'\n","test_data_dir = '/content/drive/MyDrive/Data/img/test'\n","\n","# 데이터 증강\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=10,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest',\n","    brightness_range=[0.8, 1.2]\n",")\n","\n","val_test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# 이미지 불러오기\n","train_generator = train_datagen.flow_from_directory(\n","    train_data_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical'\n",")\n","\n","validation_generator = val_test_datagen.flow_from_directory(\n","    validation_data_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical'\n",")\n","\n","test_generator = val_test_datagen.flow_from_directory(\n","    test_data_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical'\n",")"]},{"cell_type":"markdown","metadata":{"id":"-AyS03TtfQzB"},"source":["#MobileNetV1 기반 모델링(실행환경 T4)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1449,"status":"ok","timestamp":1716259564038,"user":{"displayName":"박태우","userId":"12492517778931241346"},"user_tz":-540},"id":"HBTsW46RIkF5","outputId":"f58106db-4bb4-4ee8-fe8b-bed3c89e9017"},"outputs":[{"name":"stdout","output_type":"stream","text":["패치 추출 후: (None, 1, 1, 256)\n","SE Block 후: (None, 1, 1, 256)\n","Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_6 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n","                                                                                                  \n"," mobilenet_1.00_224 (Functi  (None, 7, 7, 1024)           3228864   ['input_6[0][0]']             \n"," onal)                                                                                            \n","                                                                                                  \n"," patch_extraction (Sequenti  (None, 1, 1, 256)            411392    ['mobilenet_1.00_224[0][0]']  \n"," al)                                                                                              \n","                                                                                                  \n"," global_average_pooling2d (  (None, 256)                  0         ['patch_extraction[0][0]']    \n"," GlobalAveragePooling2D)                                                                          \n","                                                                                                  \n"," reshape (Reshape)           (None, 1, 1, 256)            0         ['global_average_pooling2d[0][\n","                                                                    0]']                          \n","                                                                                                  \n"," dense (Dense)               (None, 1, 1, 16)             4096      ['reshape[0][0]']             \n","                                                                                                  \n"," dense_1 (Dense)             (None, 1, 1, 256)            4096      ['dense[0][0]']               \n","                                                                                                  \n"," multiply (Multiply)         (None, 1, 1, 256)            0         ['patch_extraction[0][0]',    \n","                                                                     'dense_1[0][0]']             \n","                                                                                                  \n"," global_average_pooling2d_1  (None, 256)                  0         ['multiply[0][0]']            \n","  (GlobalAveragePooling2D)                                                                        \n","                                                                                                  \n"," dropout (Dropout)           (None, 256)                  0         ['global_average_pooling2d_1[0\n","                                                                    ][0]']                        \n","                                                                                                  \n"," dense_2 (Dense)             (None, 4)                    1028      ['dropout[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 3649476 (13.92 MB)\n","Trainable params: 420612 (1.60 MB)\n","Non-trainable params: 3228864 (12.32 MB)\n","__________________________________________________________________________________________________\n"]}],"source":["from tensorflow.keras.layers import SeparableConv2D,Conv2D\n","\n","def squeeze_excite_block(input_tensor, ratio=16):\n","    init = input_tensor\n","    channel_axis = 1 if tf.keras.backend.image_data_format() == \"channels_first\" else -1\n","    filters = init.shape[channel_axis]\n","    se_shape = (1, 1, filters)\n","\n","    se = GlobalAveragePooling2D()(init)\n","    se = Reshape(se_shape)(se)\n","    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n","    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n","\n","    x = tf.keras.layers.multiply([init, se])\n","    return x\n","\n","def create_mobilenet_with_attention(input_shape, num_classes):\n","    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=input_shape)\n","    base_model.trainable = False\n","\n","    inputs = Input(shape=input_shape)\n","    x = base_model(inputs, training=False)\n","\n","    # 패치 추출 레이어\n","    patch_extraction = tf.keras.Sequential([\n","        SeparableConv2D(256, kernel_size=4, strides=4, padding='same', activation='relu'),\n","        SeparableConv2D(256, kernel_size=2, strides=2, padding='valid', activation='relu'),\n","        Conv2D(256, kernel_size=1, strides=1, padding='valid', activation='relu')\n","    ], name='patch_extraction')\n","    x = patch_extraction(x)\n","    print(\"패치 추출 후:\", x.shape)  # 텐서 형태 출력\n","    x = squeeze_excite_block(x)\n","    print(\"SE Block 후:\", x.shape)  # 텐서 형태 출력\n","    x = GlobalAveragePooling2D()(x)\n","    x = Dropout(0.2)(x)\n","    outputs = Dense(num_classes, activation='softmax')(x)\n","\n","    model = Model(inputs, outputs)\n","    return model\n","\n","# 모델 생성 및 컴파일\n","model = create_mobilenet_with_attention(input_shape=(224, 224, 3), num_classes=4)\n","model.compile(optimizer=Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","\n","# 모델 구조 출력 (선택 사항)\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3063,"status":"ok","timestamp":1716259581497,"user":{"displayName":"박태우","userId":"12492517778931241346"},"user_tz":-540},"id":"abyzW-YCI-kq","outputId":"38a1efee-b618-4ce0-eac1-84e45fce4373"},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 3s 3s/step\n"]},{"data":{"text/plain":["array([[0.25012082, 0.24982189, 0.24986042, 0.2501969 ]], dtype=float32)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["sample_input = np.random.random((1, 224, 224, 3)).astype(np.float32)\n","\n","model.predict(sample_input)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"YMcnxnUKalKn","outputId":"a4e5186e-641f-428b-ae1e-c68eec6b4172"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","183/183 [==============================] - 4127s 23s/step - loss: 1.3227 - accuracy: 0.3379 - val_loss: 1.2636 - val_accuracy: 0.3853 - lr: 0.0010\n","Epoch 2/100\n","183/183 [==============================] - 452s 2s/step - loss: 1.1748 - accuracy: 0.4681 - val_loss: 1.1920 - val_accuracy: 0.4743 - lr: 0.0010\n","Epoch 3/100\n","183/183 [==============================] - 449s 2s/step - loss: 1.1197 - accuracy: 0.5044 - val_loss: 1.2220 - val_accuracy: 0.4812 - lr: 0.0010\n","Epoch 4/100\n","183/183 [==============================] - 449s 2s/step - loss: 1.0603 - accuracy: 0.5435 - val_loss: 1.1752 - val_accuracy: 0.4709 - lr: 0.0010\n","Epoch 5/100\n","183/183 [==============================] - 446s 2s/step - loss: 1.0246 - accuracy: 0.5628 - val_loss: 1.1820 - val_accuracy: 0.4692 - lr: 0.0010\n","Epoch 6/100\n","183/183 [==============================] - 457s 2s/step - loss: 0.9837 - accuracy: 0.5852 - val_loss: 1.1982 - val_accuracy: 0.4675 - lr: 0.0010\n","Epoch 7/100\n","183/183 [==============================] - 456s 2s/step - loss: 0.9088 - accuracy: 0.6236 - val_loss: 1.1952 - val_accuracy: 0.5051 - lr: 2.0000e-04\n","Epoch 8/100\n","183/183 [==============================] - 448s 2s/step - loss: 0.8774 - accuracy: 0.6366 - val_loss: 1.2103 - val_accuracy: 0.5060 - lr: 2.0000e-04\n","Epoch 9/100\n","183/183 [==============================] - 444s 2s/step - loss: 0.8583 - accuracy: 0.6540 - val_loss: 1.2198 - val_accuracy: 0.5068 - lr: 4.0000e-05\n"]}],"source":["# 학습률 조정 콜백\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6)\n","\n","# EarlyStopping 콜백\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","\n","\n","history = model.fit(train_generator, epochs=100, validation_data=validation_generator, callbacks=[reduce_lr, early_stopping])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IYiJIHEuMmcM","outputId":"471809ed-bc97-4442-e3a3-0356280b9cfb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","183/183 [==============================] - 450s 2s/step - loss: 1.0965 - accuracy: 0.5201 - val_loss: 0.9983 - val_accuracy: 0.5616 - lr: 1.0000e-04\n","Epoch 2/100\n","183/183 [==============================] - 445s 2s/step - loss: 0.9534 - accuracy: 0.5963 - val_loss: 0.9458 - val_accuracy: 0.5899 - lr: 1.0000e-04\n","Epoch 3/100\n","183/183 [==============================] - 452s 2s/step - loss: 0.8789 - accuracy: 0.6330 - val_loss: 0.9170 - val_accuracy: 0.6199 - lr: 1.0000e-04\n","Epoch 4/100\n","183/183 [==============================] - 458s 3s/step - loss: 0.8209 - accuracy: 0.6586 - val_loss: 0.8082 - val_accuracy: 0.6601 - lr: 1.0000e-04\n","Epoch 5/100\n","183/183 [==============================] - 444s 2s/step - loss: 0.7624 - accuracy: 0.6832 - val_loss: 0.7564 - val_accuracy: 0.6807 - lr: 1.0000e-04\n","Epoch 6/100\n","183/183 [==============================] - 453s 2s/step - loss: 0.7069 - accuracy: 0.7078 - val_loss: 0.8331 - val_accuracy: 0.6610 - lr: 1.0000e-04\n","Epoch 7/100\n","183/183 [==============================] - 448s 2s/step - loss: 0.6914 - accuracy: 0.7242 - val_loss: 0.8067 - val_accuracy: 0.6455 - lr: 1.0000e-04\n","Epoch 8/100\n","183/183 [==============================] - 455s 2s/step - loss: 0.5795 - accuracy: 0.7702 - val_loss: 0.7161 - val_accuracy: 0.7063 - lr: 2.0000e-05\n","Epoch 9/100\n","183/183 [==============================] - 437s 2s/step - loss: 0.5435 - accuracy: 0.7897 - val_loss: 0.7262 - val_accuracy: 0.7277 - lr: 2.0000e-05\n","Epoch 10/100\n","183/183 [==============================] - 441s 2s/step - loss: 0.5286 - accuracy: 0.7994 - val_loss: 0.7056 - val_accuracy: 0.7192 - lr: 2.0000e-05\n","Epoch 11/100\n","183/183 [==============================] - 433s 2s/step - loss: 0.5194 - accuracy: 0.7957 - val_loss: 0.7231 - val_accuracy: 0.7089 - lr: 2.0000e-05\n","Epoch 12/100\n","183/183 [==============================] - 435s 2s/step - loss: 0.5111 - accuracy: 0.8039 - val_loss: 0.6855 - val_accuracy: 0.7277 - lr: 2.0000e-05\n","Epoch 13/100\n","183/183 [==============================] - 448s 2s/step - loss: 0.4912 - accuracy: 0.8095 - val_loss: 0.7200 - val_accuracy: 0.7192 - lr: 2.0000e-05\n","Epoch 14/100\n","183/183 [==============================] - 440s 2s/step - loss: 0.4838 - accuracy: 0.8126 - val_loss: 0.7139 - val_accuracy: 0.7235 - lr: 2.0000e-05\n","Epoch 15/100\n","183/183 [==============================] - 447s 2s/step - loss: 0.4404 - accuracy: 0.8312 - val_loss: 0.6980 - val_accuracy: 0.7329 - lr: 4.0000e-06\n","Epoch 16/100\n","183/183 [==============================] - 441s 2s/step - loss: 0.4383 - accuracy: 0.8315 - val_loss: 0.6999 - val_accuracy: 0.7337 - lr: 4.0000e-06\n","Epoch 17/100\n","183/183 [==============================] - 444s 2s/step - loss: 0.4314 - accuracy: 0.8336 - val_loss: 0.7013 - val_accuracy: 0.7337 - lr: 1.0000e-06\n"]}],"source":["# 파인튜닝 과정\n","base_model = model.layers[1]\n","base_model.trainable = True\n","\n","# 사전 학습된 모델의 전체 레이어 갯수 가져오기\n","num_layers = len(base_model.layers)\n","\n","# 훈련되지 않고 고정될 레이어 비율\n","freeze_ratio = 0.6\n","num_freeze_layers = int(num_layers * freeze_ratio)\n","\n","# 하위 레이어 고정, 상위 레이어 해제\n","for layer in base_model.layers[:num_freeze_layers]:\n","    layer.trainable = False\n","for layer in base_model.layers[num_freeze_layers:]:\n","    layer.trainable = True\n","\n","# 모델 재컴파일\n","model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# 모델 학습\n","history_fine = model.fit(train_generator, epochs=100, validation_data=validation_generator, callbacks=[reduce_lr, early_stopping])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"R0vRn2p3Z3xJ","outputId":"b80e4e2b-92ad-4318-c461-5bd0931acf83"},"outputs":[{"name":"stdout","output_type":"stream","text":["37/37 [==============================] - 62s 2s/step - loss: 0.6855 - accuracy: 0.7277\n","Validation Loss: 0.6855243444442749\n","Validation Accuracy: 0.7277397513389587\n"]}],"source":["# 검증 데이터 확인\n","loss, accuracy = model.evaluate(validation_generator)\n","print(f\"Validation Loss: {loss}\")\n","print(f\"Validation Accuracy: {accuracy}\")"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"sB5d2u6qsafF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716275479546,"user_tz":-540,"elapsed":490844,"user":{"displayName":"박태우","userId":"12492517778931241346"}},"outputId":"be4e0670-c549-43e4-f3cc-79c0f8be1bfb"},"outputs":[{"output_type":"stream","name":"stdout","text":["36/36 [==============================] - 489s 14s/step - loss: 0.6840 - accuracy: 0.7291\n","Test Loss: 0.6839980483055115\n","Test Accuracy: 0.7291116714477539\n"]}],"source":["# 테스트 데이터 전체 검증\n","test_loss, test_accuracy = model.evaluate(test_generator)\n","print(f\"Test Loss: {test_loss}\")\n","print(f\"Test Accuracy: {test_accuracy}\")\n"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"gu7AzcLndjh3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716275479883,"user_tz":-540,"elapsed":341,"user":{"displayName":"박태우","userId":"12492517778931241346"}},"outputId":"76aaada9-96c7-4470-d360-bfaaccde41f2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}],"source":["# 모델 저장\n","model.save('/content/drive/MyDrive/MobileNetV1_base.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J1E-s7AEBlW8"},"outputs":[],"source":["# 결과 그래프 출력\n","acc = history.history['accuracy'] + history_fine.history['accuracy']\n","val_acc = history.history['val_accuracy'] + history_fine.history['val_accuracy']\n","\n","loss = history.history['loss'] + history_fine.history['loss']\n","val_loss = history.history['val_loss'] + history_fine.history['val_loss']\n","\n","plt.figure(figsize=(8, 8))\n","plt.subplot(2, 1, 1)\n","plt.plot(acc, label='Training Accuracy')\n","plt.plot(val_acc, label='Validation Accuracy')\n","plt.legend(loc='lower right')\n","plt.ylabel('Accuracy')\n","plt.ylim([min(plt.ylim()), 1])\n","plt.title('Training and Validation Accuracy')\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(loss, label='Training Loss')\n","plt.plot(val_loss, label='Validation Loss')\n","plt.legend(loc='upper right')\n","plt.ylabel('Cross Entropy')\n","plt.ylim([0, 1.0])\n","plt.title('Training and Validation Loss')\n","plt.xlabel('epoch')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"p5kkjYL9WQnh"},"source":["#임의의 이미지로 모델 성능 test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3737,"status":"ok","timestamp":1715746283111,"user":{"displayName":"이스트캠퍼스","userId":"11506478735001745454"},"user_tz":-540},"id":"Mo87OeFDBahU","outputId":"ec174cc1-5557-45d2-9946-c144e7007d72"},"outputs":[{"name":"stdout","output_type":"stream","text":["이미지 로딩 중...\n","감정 예측 중...\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x78f1b126f1c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 1s 704ms/step\n","예측된 감정: 행복\n","각 감정의 비율:\n","화남: 4.20%\n","행복: 79.71%\n","슬픔: 1.21%\n","충격: 14.88%\n"]}],"source":["# 이미지 전처리 함수\n","def load_and_prepare_image(image_path, target_size=(224, 224)):\n","    img = image.load_img(image_path, target_size=target_size)\n","    img_tensor = image.img_to_array(img)\n","    img_tensor = np.expand_dims(img_tensor, axis=0)\n","    img_tensor /= 255.0  # 정규화\n","    return img_tensor\n","\n","# 감정을 예측하는 함수\n","def predict_emotion(model, img_path):\n","    print(\"이미지 로딩 중...\")\n","    test_image = load_and_prepare_image(img_path)\n","    print(\"감정 예측 중...\")\n","    prediction = model.predict(test_image)\n","    return prediction\n","\n","# 모델 경로와 이미지 경로\n","model_path = '/content/my_model_MobileNetV2.h5'  # 학습된 모델 경로\n","image_path = '/content/drive/MyDrive/test/1.jpg'  # 테스트할 이미지 경로\n","\n","# 모델 로드\n","model = load_model(model_path)\n","\n","# 예측 결과 얻기\n","predictions = predict_emotion(model, image_path)\n","emotion_index = np.argmax(predictions)\n","emotion_labels = ['화남', '행복', '슬픔', '충격']  # 실제 감정 레이블로 변경\n","\n","print(\"예측된 감정:\", emotion_labels[emotion_index])\n","print(\"각 감정의 비율:\")\n","for i, label in enumerate(emotion_labels):\n","    print(f\"{label}: {predictions[0][i] * 100:.2f}%\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}