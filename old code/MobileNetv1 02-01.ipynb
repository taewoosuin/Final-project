{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21684,"status":"ok","timestamp":1716268830222,"user":{"displayName":"박태우","userId":"12492517778931241346"},"user_tz":-540},"id":"xV7z-yIHva2K","outputId":"666471e9-632b-4f40-9a79-d77c2778b880"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"in6uBGa3pHn_"},"outputs":[],"source":["!pip install --upgrade tensorflow\n"]},{"cell_type":"markdown","metadata":{"id":"ECxNcYjjUaKH"},"source":["#데이터 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4nI_ceMCZw0N"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dropout, Dense, Reshape, SeparableConv2D, Conv2D, BatchNormalization, Multiply, Layer,Attention, LayerNormalization, Add\n","from tensorflow.keras.applications import MobileNet\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15587,"status":"ok","timestamp":1716268848681,"user":{"displayName":"박태우","userId":"12492517778931241346"},"user_tz":-540},"id":"0C-5LhNHJapI","outputId":"e4f6a507-94c7-4d6b-fd9d-e538e33e8c47"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 5853 images belonging to 4 classes.\n","Found 1168 images belonging to 4 classes.\n","Found 1137 images belonging to 4 classes.\n"]}],"source":["# 데이터 경로\n","train_data_dir = '/content/drive/MyDrive/Data/img/train'\n","validation_data_dir = '/content/drive/MyDrive/Data/img/val'\n","test_data_dir = '/content/drive/MyDrive/Data/img/test'\n","\n","# 데이터 증강\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,\n","    rotation_range=10,\n","    width_shift_range=0.1,\n","    height_shift_range=0.1,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest',\n","    brightness_range=[0.8, 1.2]\n",")\n","\n","val_test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# 이미지 불러오기\n","train_generator = train_datagen.flow_from_directory(\n","    train_data_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical'\n",")\n","\n","validation_generator = val_test_datagen.flow_from_directory(\n","    validation_data_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical'\n",")\n","\n","test_generator = val_test_datagen.flow_from_directory(\n","    test_data_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical'\n",")"]},{"cell_type":"markdown","metadata":{"id":"-AyS03TtfQzB"},"source":["#MobileNetV1 기반 모델링(실행환경 T4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HBTsW46RIkF5"},"outputs":[],"source":["# Base model 정의\n","def create_base_model(input_shape):\n","    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=input_shape)\n","    base_model.trainable = False\n","    return base_model\n","\n","# SE 블록\n","def squeeze_excite_block(input_tensor, ratio=16):\n","    init = input_tensor\n","    channel_axis = 1 if tf.keras.backend.image_data_format() == \"channels_first\" else -1\n","    filters = init.shape[channel_axis]\n","    se_shape = (1, 1, filters)\n","\n","    se = GlobalAveragePooling2D()(init)\n","    se = Reshape(se_shape)(se)\n","    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n","    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n","\n","    x = Multiply()([init, se])\n","    return x\n","\n","# CBAM 블록\n","class CBAMBlock(Layer):\n","    def __init__(self, ratio=8, **kwargs):\n","        super(CBAMBlock, self).__init__(**kwargs)\n","        self.ratio = ratio\n","\n","    def build(self, input_shape):\n","        self.channel_dense_1 = Dense(input_shape[-1] // self.ratio, activation='relu', kernel_initializer='he_normal', use_bias=True, bias_initializer='zeros')\n","        self.channel_dense_2 = Dense(input_shape[-1], activation='sigmoid', kernel_initializer='he_normal', use_bias=True, bias_initializer='zeros')\n","        self.spatial_conv = Conv2D(filters=1, kernel_size=7, padding='same', activation='sigmoid', kernel_initializer='he_normal', use_bias=False)\n","\n","    def call(self, inputs):\n","        # Channel attention\n","        channel = GlobalAveragePooling2D()(inputs)\n","        channel = Reshape((1, 1, inputs.shape[-1]))(channel)\n","        channel = self.channel_dense_1(channel)\n","        channel = self.channel_dense_2(channel)\n","        channel_attention = Multiply()([inputs, channel])\n","\n","        # Spatial attention\n","        spatial_avg = tf.reduce_mean(channel_attention, axis=-1, keepdims=True)\n","        spatial_max = tf.reduce_max(channel_attention, axis=-1, keepdims=True)\n","        spatial_attention = tf.concat([spatial_avg, spatial_max], axis=-1)\n","        spatial_attention = self.spatial_conv(spatial_attention)\n","        output_tensor = Multiply()([channel_attention, spatial_attention])\n","\n","        return output_tensor\n","\n","# 패치 추출 레이어 정의\n","def patch_extraction_layer():\n","    return tf.keras.Sequential([\n","        SeparableConv2D(256, kernel_size=4, strides=4, padding='same', activation='relu'),\n","        SeparableConv2D(256, kernel_size=2, strides=2, padding='valid', activation='relu'),\n","        Conv2D(256, kernel_size=1, strides=1, padding='valid', activation='relu')\n","    ], name='patch_extraction')\n","\n","# Pre-classification 레이어 정의\n","def create_pre_classification_layer():\n","    return tf.keras.Sequential([\n","        Dense(32, activation='relu'),\n","        BatchNormalization()\n","    ], name='pre_classification')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1435,"status":"ok","timestamp":1716269651104,"user":{"displayName":"박태우","userId":"12492517778931241346"},"user_tz":-540},"id":"Ba41_ozA4V70","outputId":"6010d0f7-ba6a-48da-edbc-d9a6e0f44613"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_10\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_28 (InputLayer)       [(None, 224, 224, 3)]        0         []                            \n","                                                                                                  \n"," mobilenet_1.00_224 (Functi  (None, 7, 7, 1024)           3228864   ['input_28[0][0]']            \n"," onal)                                                                                            \n","                                                                                                  \n"," patch_extraction (Sequenti  (None, 1, 1, 256)            411392    ['mobilenet_1.00_224[0][0]']  \n"," al)                                                                                              \n","                                                                                                  \n"," gap (GlobalAveragePooling2  (None, 256)                  0         ['patch_extraction[0][0]']    \n"," D)                                                                                               \n","                                                                                                  \n"," dropout_13 (Dropout)        (None, 256)                  0         ['gap[0][0]']                 \n","                                                                                                  \n"," pre_classification (Sequen  (None, 32)                   8352      ['dropout_13[0][0]']          \n"," tial)                                                                                            \n","                                                                                                  \n"," attention_8 (Attention)     (None, 32)                   1         ['pre_classification[0][0]',  \n","                                                                     'pre_classification[0][0]']  \n","                                                                                                  \n"," add_4 (Add)                 (None, 32)                   0         ['pre_classification[0][0]',  \n","                                                                     'attention_8[0][0]']         \n","                                                                                                  \n"," dense_24 (Dense)            (None, 4)                    132       ['add_4[0][0]']               \n","                                                                                                  \n","==================================================================================================\n","Total params: 3648741 (13.92 MB)\n","Trainable params: 419813 (1.60 MB)\n","Non-trainable params: 3228928 (12.32 MB)\n","__________________________________________________________________________________________________\n"]}],"source":["def create_mobilenet_with_cbam_block(input_shape, num_classes):\n","    base_model = create_base_model(input_shape)\n","    inputs = Input(shape=input_shape)\n","    # 기본 모델\n","    x = base_model(inputs, training=False)\n","    # 패치 추출 레이어\n","    x = patch_extraction_layer()(x)\n","    # GlobalAveragePooling2D 및 Dropout\n","    x = GlobalAveragePooling2D(name='gap')(x)\n","    x = Dropout(0.2)(x)\n","    # 사전 분류 레이어\n","    x = create_pre_classification_layer()(x)\n","    # Self-Attention 추가\n","    attn_output = Attention(use_scale=True)([x, x])\n","    x = Add()([x, attn_output])\n","    # 출력층\n","    outputs = Dense(num_classes, activation='softmax')(x)\n","    model = Model(inputs, outputs)\n","    return model\n","\n","\n","# 모델 생성 및 컴파일\n","model = create_mobilenet_with_cbam_block(input_shape=(224, 224, 3), num_classes=4)\n","model.compile(optimizer=Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# 모델 요약 출력\n","model.summary()"]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# MobileNet 모델 로드\n","base_model = tf.keras.applications.MobileNet(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n","\n","# 모델 레이어 목록 출력\n","for i, layer in enumerate(base_model.layers):\n","    print(f\"{i}: {layer.name}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"RIopnJsDC6hh","executionInfo":{"status":"ok","timestamp":1716293189365,"user_tz":-540,"elapsed":3011,"user":{"displayName":"박태우","userId":"12492517778931241346"}},"outputId":"f5bd2610-19e4-4269-c9ce-33b9087fe98e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n","17225924/17225924 [==============================] - 1s 0us/step\n","0: input_1\n","1: conv1\n","2: conv1_bn\n","3: conv1_relu\n","4: conv_dw_1\n","5: conv_dw_1_bn\n","6: conv_dw_1_relu\n","7: conv_pw_1\n","8: conv_pw_1_bn\n","9: conv_pw_1_relu\n","10: conv_pad_2\n","11: conv_dw_2\n","12: conv_dw_2_bn\n","13: conv_dw_2_relu\n","14: conv_pw_2\n","15: conv_pw_2_bn\n","16: conv_pw_2_relu\n","17: conv_dw_3\n","18: conv_dw_3_bn\n","19: conv_dw_3_relu\n","20: conv_pw_3\n","21: conv_pw_3_bn\n","22: conv_pw_3_relu\n","23: conv_pad_4\n","24: conv_dw_4\n","25: conv_dw_4_bn\n","26: conv_dw_4_relu\n","27: conv_pw_4\n","28: conv_pw_4_bn\n","29: conv_pw_4_relu\n","30: conv_dw_5\n","31: conv_dw_5_bn\n","32: conv_dw_5_relu\n","33: conv_pw_5\n","34: conv_pw_5_bn\n","35: conv_pw_5_relu\n","36: conv_pad_6\n","37: conv_dw_6\n","38: conv_dw_6_bn\n","39: conv_dw_6_relu\n","40: conv_pw_6\n","41: conv_pw_6_bn\n","42: conv_pw_6_relu\n","43: conv_dw_7\n","44: conv_dw_7_bn\n","45: conv_dw_7_relu\n","46: conv_pw_7\n","47: conv_pw_7_bn\n","48: conv_pw_7_relu\n","49: conv_dw_8\n","50: conv_dw_8_bn\n","51: conv_dw_8_relu\n","52: conv_pw_8\n","53: conv_pw_8_bn\n","54: conv_pw_8_relu\n","55: conv_dw_9\n","56: conv_dw_9_bn\n","57: conv_dw_9_relu\n","58: conv_pw_9\n","59: conv_pw_9_bn\n","60: conv_pw_9_relu\n","61: conv_dw_10\n","62: conv_dw_10_bn\n","63: conv_dw_10_relu\n","64: conv_pw_10\n","65: conv_pw_10_bn\n","66: conv_pw_10_relu\n","67: conv_dw_11\n","68: conv_dw_11_bn\n","69: conv_dw_11_relu\n","70: conv_pw_11\n","71: conv_pw_11_bn\n","72: conv_pw_11_relu\n","73: conv_pad_12\n","74: conv_dw_12\n","75: conv_dw_12_bn\n","76: conv_dw_12_relu\n","77: conv_pw_12\n","78: conv_pw_12_bn\n","79: conv_pw_12_relu\n","80: conv_dw_13\n","81: conv_dw_13_bn\n","82: conv_dw_13_relu\n","83: conv_pw_13\n","84: conv_pw_13_bn\n","85: conv_pw_13_relu\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I889xBoKM88_","colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"status":"ok","timestamp":1716293292551,"user_tz":-540,"elapsed":466,"user":{"displayName":"박태우","userId":"12492517778931241346"}},"outputId":"eaafd50a-dc09-42df-f1b8-ce2bbd59a8c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"mobilenet_1.00_224\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," conv1 (Conv2D)              (None, 112, 112, 32)      864       \n","                                                                 \n"," conv1_bn (BatchNormalizati  (None, 112, 112, 32)      128       \n"," on)                                                             \n","                                                                 \n"," conv1_relu (ReLU)           (None, 112, 112, 32)      0         \n","                                                                 \n"," conv_dw_1 (DepthwiseConv2D  (None, 112, 112, 32)      288       \n"," )                                                               \n","                                                                 \n"," conv_dw_1_bn (BatchNormali  (None, 112, 112, 32)      128       \n"," zation)                                                         \n","                                                                 \n"," conv_dw_1_relu (ReLU)       (None, 112, 112, 32)      0         \n","                                                                 \n"," conv_pw_1 (Conv2D)          (None, 112, 112, 64)      2048      \n","                                                                 \n"," conv_pw_1_bn (BatchNormali  (None, 112, 112, 64)      256       \n"," zation)                                                         \n","                                                                 \n"," conv_pw_1_relu (ReLU)       (None, 112, 112, 64)      0         \n","                                                                 \n"," conv_pad_2 (ZeroPadding2D)  (None, 113, 113, 64)      0         \n","                                                                 \n"," conv_dw_2 (DepthwiseConv2D  (None, 56, 56, 64)        576       \n"," )                                                               \n","                                                                 \n"," conv_dw_2_bn (BatchNormali  (None, 56, 56, 64)        256       \n"," zation)                                                         \n","                                                                 \n"," conv_dw_2_relu (ReLU)       (None, 56, 56, 64)        0         \n","                                                                 \n"," conv_pw_2 (Conv2D)          (None, 56, 56, 128)       8192      \n","                                                                 \n"," conv_pw_2_bn (BatchNormali  (None, 56, 56, 128)       512       \n"," zation)                                                         \n","                                                                 \n"," conv_pw_2_relu (ReLU)       (None, 56, 56, 128)       0         \n","                                                                 \n"," conv_dw_3 (DepthwiseConv2D  (None, 56, 56, 128)       1152      \n"," )                                                               \n","                                                                 \n"," conv_dw_3_bn (BatchNormali  (None, 56, 56, 128)       512       \n"," zation)                                                         \n","                                                                 \n"," conv_dw_3_relu (ReLU)       (None, 56, 56, 128)       0         \n","                                                                 \n"," conv_pw_3 (Conv2D)          (None, 56, 56, 128)       16384     \n","                                                                 \n"," conv_pw_3_bn (BatchNormali  (None, 56, 56, 128)       512       \n"," zation)                                                         \n","                                                                 \n"," conv_pw_3_relu (ReLU)       (None, 56, 56, 128)       0         \n","                                                                 \n"," conv_pad_4 (ZeroPadding2D)  (None, 57, 57, 128)       0         \n","                                                                 \n"," conv_dw_4 (DepthwiseConv2D  (None, 28, 28, 128)       1152      \n"," )                                                               \n","                                                                 \n"," conv_dw_4_bn (BatchNormali  (None, 28, 28, 128)       512       \n"," zation)                                                         \n","                                                                 \n"," conv_dw_4_relu (ReLU)       (None, 28, 28, 128)       0         \n","                                                                 \n"," conv_pw_4 (Conv2D)          (None, 28, 28, 256)       32768     \n","                                                                 \n"," conv_pw_4_bn (BatchNormali  (None, 28, 28, 256)       1024      \n"," zation)                                                         \n","                                                                 \n"," conv_pw_4_relu (ReLU)       (None, 28, 28, 256)       0         \n","                                                                 \n"," conv_dw_5 (DepthwiseConv2D  (None, 28, 28, 256)       2304      \n"," )                                                               \n","                                                                 \n"," conv_dw_5_bn (BatchNormali  (None, 28, 28, 256)       1024      \n"," zation)                                                         \n","                                                                 \n"," conv_dw_5_relu (ReLU)       (None, 28, 28, 256)       0         \n","                                                                 \n"," conv_pw_5 (Conv2D)          (None, 28, 28, 256)       65536     \n","                                                                 \n"," conv_pw_5_bn (BatchNormali  (None, 28, 28, 256)       1024      \n"," zation)                                                         \n","                                                                 \n"," conv_pw_5_relu (ReLU)       (None, 28, 28, 256)       0         \n","                                                                 \n"," conv_pad_6 (ZeroPadding2D)  (None, 29, 29, 256)       0         \n","                                                                 \n"," conv_dw_6 (DepthwiseConv2D  (None, 14, 14, 256)       2304      \n"," )                                                               \n","                                                                 \n"," conv_dw_6_bn (BatchNormali  (None, 14, 14, 256)       1024      \n"," zation)                                                         \n","                                                                 \n"," conv_dw_6_relu (ReLU)       (None, 14, 14, 256)       0         \n","                                                                 \n"," conv_pw_6 (Conv2D)          (None, 14, 14, 512)       131072    \n","                                                                 \n"," conv_pw_6_bn (BatchNormali  (None, 14, 14, 512)       2048      \n"," zation)                                                         \n","                                                                 \n"," conv_pw_6_relu (ReLU)       (None, 14, 14, 512)       0         \n","                                                                 \n"," conv_dw_7 (DepthwiseConv2D  (None, 14, 14, 512)       4608      \n"," )                                                               \n","                                                                 \n"," conv_dw_7_bn (BatchNormali  (None, 14, 14, 512)       2048      \n"," zation)                                                         \n","                                                                 \n"," conv_dw_7_relu (ReLU)       (None, 14, 14, 512)       0         \n","                                                                 \n"," conv_pw_7 (Conv2D)          (None, 14, 14, 512)       262144    \n","                                                                 \n"," conv_pw_7_bn (BatchNormali  (None, 14, 14, 512)       2048      \n"," zation)                                                         \n","                                                                 \n"," conv_pw_7_relu (ReLU)       (None, 14, 14, 512)       0         \n","                                                                 \n"," conv_dw_8 (DepthwiseConv2D  (None, 14, 14, 512)       4608      \n"," )                                                               \n","                                                                 \n"," conv_dw_8_bn (BatchNormali  (None, 14, 14, 512)       2048      \n"," zation)                                                         \n","                                                                 \n"," conv_dw_8_relu (ReLU)       (None, 14, 14, 512)       0         \n","                                                                 \n"," conv_pw_8 (Conv2D)          (None, 14, 14, 512)       262144    \n","                                                                 \n"," conv_pw_8_bn (BatchNormali  (None, 14, 14, 512)       2048      \n"," zation)                                                         \n","                                                                 \n"," conv_pw_8_relu (ReLU)       (None, 14, 14, 512)       0         \n","                                                                 \n"," conv_dw_9 (DepthwiseConv2D  (None, 14, 14, 512)       4608      \n"," )                                                               \n","                                                                 \n"," conv_dw_9_bn (BatchNormali  (None, 14, 14, 512)       2048      \n"," zation)                                                         \n","                                                                 \n"," conv_dw_9_relu (ReLU)       (None, 14, 14, 512)       0         \n","                                                                 \n"," conv_pw_9 (Conv2D)          (None, 14, 14, 512)       262144    \n","                                                                 \n"," conv_pw_9_bn (BatchNormali  (None, 14, 14, 512)       2048      \n"," zation)                                                         \n","                                                                 \n"," conv_pw_9_relu (ReLU)       (None, 14, 14, 512)       0         \n","                                                                 \n"," conv_dw_10 (DepthwiseConv2  (None, 14, 14, 512)       4608      \n"," D)                                                              \n","                                                                 \n"," conv_dw_10_bn (BatchNormal  (None, 14, 14, 512)       2048      \n"," ization)                                                        \n","                                                                 \n"," conv_dw_10_relu (ReLU)      (None, 14, 14, 512)       0         \n","                                                                 \n"," conv_pw_10 (Conv2D)         (None, 14, 14, 512)       262144    \n","                                                                 \n"," conv_pw_10_bn (BatchNormal  (None, 14, 14, 512)       2048      \n"," ization)                                                        \n","                                                                 \n"," conv_pw_10_relu (ReLU)      (None, 14, 14, 512)       0         \n","                                                                 \n"," conv_dw_11 (DepthwiseConv2  (None, 14, 14, 512)       4608      \n"," D)                                                              \n","                                                                 \n"," conv_dw_11_bn (BatchNormal  (None, 14, 14, 512)       2048      \n"," ization)                                                        \n","                                                                 \n"," conv_dw_11_relu (ReLU)      (None, 14, 14, 512)       0         \n","                                                                 \n"," conv_pw_11 (Conv2D)         (None, 14, 14, 512)       262144    \n","                                                                 \n"," conv_pw_11_bn (BatchNormal  (None, 14, 14, 512)       2048      \n"," ization)                                                        \n","                                                                 \n"," conv_pw_11_relu (ReLU)      (None, 14, 14, 512)       0         \n","                                                                 \n"," conv_pad_12 (ZeroPadding2D  (None, 15, 15, 512)       0         \n"," )                                                               \n","                                                                 \n"," conv_dw_12 (DepthwiseConv2  (None, 7, 7, 512)         4608      \n"," D)                                                              \n","                                                                 \n"," conv_dw_12_bn (BatchNormal  (None, 7, 7, 512)         2048      \n"," ization)                                                        \n","                                                                 \n"," conv_dw_12_relu (ReLU)      (None, 7, 7, 512)         0         \n","                                                                 \n"," conv_pw_12 (Conv2D)         (None, 7, 7, 1024)        524288    \n","                                                                 \n"," conv_pw_12_bn (BatchNormal  (None, 7, 7, 1024)        4096      \n"," ization)                                                        \n","                                                                 \n"," conv_pw_12_relu (ReLU)      (None, 7, 7, 1024)        0         \n","                                                                 \n"," conv_dw_13 (DepthwiseConv2  (None, 7, 7, 1024)        9216      \n"," D)                                                              \n","                                                                 \n"," conv_dw_13_bn (BatchNormal  (None, 7, 7, 1024)        4096      \n"," ization)                                                        \n","                                                                 \n"," conv_dw_13_relu (ReLU)      (None, 7, 7, 1024)        0         \n","                                                                 \n"," conv_pw_13 (Conv2D)         (None, 7, 7, 1024)        1048576   \n","                                                                 \n"," conv_pw_13_bn (BatchNormal  (None, 7, 7, 1024)        4096      \n"," ization)                                                        \n","                                                                 \n"," conv_pw_13_relu (ReLU)      (None, 7, 7, 1024)        0         \n","                                                                 \n","=================================================================\n","Total params: 3228864 (12.32 MB)\n","Trainable params: 3206976 (12.23 MB)\n","Non-trainable params: 21888 (85.50 KB)\n","_________________________________________________________________\n"]}],"source":["# base_model의 구조를 출력\n","base_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qQdn9ZuV0CeI"},"outputs":[],"source":["# 학습률 조정 콜백\n","reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=2, min_delta=0.005, min_lr=1e-7)\n","\n","# EarlyStopping 콜백\n","early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, min_delta=0.005, restore_best_weights=True)\n","\n","# 클래스 가중치 추가\n","train_samples_per_class = np.array([1440, 1489, 1477, 1447])\n","total_train_samples = np.sum(train_samples_per_class)\n","class_weights = {i: total_train_samples / (len(train_samples_per_class) * count) for i, count in enumerate(train_samples_per_class)}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"F9Qda9o00Ex4","outputId":"da03a51d-9ac4-4c54-f4b1-53e2e1091030"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","183/183 [==============================] - 5314s 29s/step - loss: 1.3129 - accuracy: 0.3665 - val_loss: 1.3728 - val_accuracy: 0.2594 - lr: 0.0010\n","Epoch 2/100\n","183/183 [==============================] - 448s 2s/step - loss: 1.2101 - accuracy: 0.4596 - val_loss: 1.3371 - val_accuracy: 0.3502 - lr: 0.0010\n","Epoch 3/100\n","183/183 [==============================] - 449s 2s/step - loss: 1.1382 - accuracy: 0.5013 - val_loss: 1.2863 - val_accuracy: 0.3887 - lr: 0.0010\n","Epoch 4/100\n","183/183 [==============================] - 447s 2s/step - loss: 1.1033 - accuracy: 0.5238 - val_loss: 1.3833 - val_accuracy: 0.3930 - lr: 0.0010\n","Epoch 5/100\n","183/183 [==============================] - 449s 2s/step - loss: 1.0688 - accuracy: 0.5440 - val_loss: 1.4659 - val_accuracy: 0.4221 - lr: 0.0010\n","Epoch 6/100\n","183/183 [==============================] - 445s 2s/step - loss: 1.0398 - accuracy: 0.5623 - val_loss: 1.3285 - val_accuracy: 0.4384 - lr: 0.0010\n","Epoch 7/100\n","183/183 [==============================] - 447s 2s/step - loss: 1.0199 - accuracy: 0.5775 - val_loss: 1.4027 - val_accuracy: 0.4435 - lr: 0.0010\n","Epoch 8/100\n","183/183 [==============================] - 441s 2s/step - loss: 0.9921 - accuracy: 0.5900 - val_loss: 1.2850 - val_accuracy: 0.4692 - lr: 0.0010\n","Epoch 9/100\n","183/183 [==============================] - 432s 2s/step - loss: 0.9777 - accuracy: 0.5995 - val_loss: 1.2847 - val_accuracy: 0.4906 - lr: 0.0010\n","Epoch 10/100\n","183/183 [==============================] - 429s 2s/step - loss: 0.9430 - accuracy: 0.6161 - val_loss: 1.2957 - val_accuracy: 0.4572 - lr: 0.0010\n","Epoch 11/100\n","183/183 [==============================] - 442s 2s/step - loss: 0.9273 - accuracy: 0.6173 - val_loss: 1.2732 - val_accuracy: 0.4666 - lr: 0.0010\n","Epoch 12/100\n","183/183 [==============================] - 444s 2s/step - loss: 0.8484 - accuracy: 0.6658 - val_loss: 1.2736 - val_accuracy: 0.4837 - lr: 2.0000e-04\n","Epoch 13/100\n","183/183 [==============================] - 438s 2s/step - loss: 0.8211 - accuracy: 0.6785 - val_loss: 1.2877 - val_accuracy: 0.4752 - lr: 2.0000e-04\n","Epoch 14/100\n","183/183 [==============================] - 442s 2s/step - loss: 0.7901 - accuracy: 0.6882 - val_loss: 1.2500 - val_accuracy: 0.4932 - lr: 4.0000e-05\n"]}],"source":["# 모델 훈련\n","history = model.fit(\n","    train_generator,\n","    epochs=100,\n","    validation_data=validation_generator,\n","    callbacks=[reduce_lr, early_stopping],\n","    class_weight=class_weights\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"IYiJIHEuMmcM","outputId":"899a9391-4f06-4cbf-b688-7f134583ec47"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_10\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_28 (InputLayer)       [(None, 224, 224, 3)]        0         []                            \n","                                                                                                  \n"," mobilenet_1.00_224 (Functi  (None, 7, 7, 1024)           3228864   ['input_28[0][0]']            \n"," onal)                                                                                            \n","                                                                                                  \n"," patch_extraction (Sequenti  (None, None, None, 256)      411392    ['mobilenet_1.00_224[0][0]']  \n"," al)                                                                                              \n","                                                                                                  \n"," gap (GlobalAveragePooling2  (None, 256)                  0         ['patch_extraction[0][0]']    \n"," D)                                                                                               \n","                                                                                                  \n"," dropout_13 (Dropout)        (None, 256)                  0         ['gap[0][0]']                 \n","                                                                                                  \n"," pre_classification (Sequen  (None, 32)                   8352      ['dropout_13[0][0]']          \n"," tial)                                                                                            \n","                                                                                                  \n"," attention_8 (Attention)     (None, 32)                   1         ['pre_classification[0][0]',  \n","                                                                     'pre_classification[0][0]']  \n","                                                                                                  \n"," add_4 (Add)                 (None, 32)                   0         ['pre_classification[0][0]',  \n","                                                                     'attention_8[0][0]']         \n","                                                                                                  \n"," dense_24 (Dense)            (None, 4)                    132       ['add_4[0][0]']               \n","                                                                                                  \n","==================================================================================================\n","Total params: 3648741 (13.92 MB)\n","Trainable params: 3340261 (12.74 MB)\n","Non-trainable params: 308480 (1.18 MB)\n","__________________________________________________________________________________________________\n"]}],"source":["# 파인튜닝 과정\n","base_model = model.layers[1]\n","base_model.trainable = True\n","\n","# 사전 학습된 모델의 전체 레이어 갯수\n","num_layers = len(base_model.layers)\n","\n","# 훈련되지 않고 고정될 레이어 비율\n","freeze_ratio = 0.5\n","num_freeze_layers = int(num_layers * freeze_ratio)\n","\n","# 하위 레이어 고정, 상위 레이어 해제\n","for layer in base_model.layers[:num_freeze_layers]:\n","    layer.trainable = False\n","for layer in base_model.layers[num_freeze_layers:]:\n","    if isinstance(layer, BatchNormalization):\n","        layer.trainable = False\n","    else:\n","        layer.trainable = True\n","\n","# 모델 재컴파일\n","model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# 모델 요약 출력\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qG5UtjAI11_K","outputId":"6ec2cdc5-ec77-4614-e812-f18092bc0df2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","183/183 [==============================] - 448s 2s/step - loss: 1.0941 - accuracy: 0.5308 - val_loss: 1.2189 - val_accuracy: 0.4914 - lr: 1.0000e-04\n","Epoch 2/100\n","183/183 [==============================] - 443s 2s/step - loss: 0.9495 - accuracy: 0.6031 - val_loss: 1.1738 - val_accuracy: 0.5479 - lr: 1.0000e-04\n","Epoch 3/100\n","183/183 [==============================] - 441s 2s/step - loss: 0.8644 - accuracy: 0.6492 - val_loss: 1.0985 - val_accuracy: 0.5582 - lr: 1.0000e-04\n","Epoch 4/100\n","183/183 [==============================] - 454s 2s/step - loss: 0.8193 - accuracy: 0.6658 - val_loss: 1.0431 - val_accuracy: 0.5642 - lr: 1.0000e-04\n","Epoch 5/100\n","183/183 [==============================] - 438s 2s/step - loss: 0.7653 - accuracy: 0.6971 - val_loss: 0.9874 - val_accuracy: 0.6156 - lr: 1.0000e-04\n","Epoch 6/100\n","183/183 [==============================] - 436s 2s/step - loss: 0.7354 - accuracy: 0.7107 - val_loss: 0.9337 - val_accuracy: 0.6310 - lr: 1.0000e-04\n","Epoch 7/100\n","183/183 [==============================] - 431s 2s/step - loss: 0.7177 - accuracy: 0.7167 - val_loss: 1.0453 - val_accuracy: 0.6182 - lr: 1.0000e-04\n","Epoch 8/100\n","183/183 [==============================] - 440s 2s/step - loss: 0.6723 - accuracy: 0.7413 - val_loss: 1.0087 - val_accuracy: 0.6404 - lr: 1.0000e-04\n","Epoch 9/100\n","183/183 [==============================] - 452s 2s/step - loss: 0.6171 - accuracy: 0.7729 - val_loss: 1.0718 - val_accuracy: 0.6353 - lr: 1.0000e-04\n","Epoch 10/100\n","183/183 [==============================] - 480s 3s/step - loss: 0.5959 - accuracy: 0.7719 - val_loss: 1.1437 - val_accuracy: 0.6156 - lr: 1.0000e-04\n","Epoch 11/100\n","183/183 [==============================] - 476s 3s/step - loss: 0.5422 - accuracy: 0.7957 - val_loss: 0.7076 - val_accuracy: 0.7414 - lr: 2.0000e-05\n","Epoch 12/100\n","183/183 [==============================] - 480s 3s/step - loss: 0.5077 - accuracy: 0.8189 - val_loss: 0.7866 - val_accuracy: 0.7089 - lr: 2.0000e-05\n","Epoch 13/100\n","183/183 [==============================] - 476s 3s/step - loss: 0.4954 - accuracy: 0.8256 - val_loss: 0.7356 - val_accuracy: 0.7209 - lr: 2.0000e-05\n","Epoch 14/100\n","183/183 [==============================] - 489s 3s/step - loss: 0.4779 - accuracy: 0.8302 - val_loss: 0.6610 - val_accuracy: 0.7500 - lr: 4.0000e-06\n","Epoch 15/100\n","183/183 [==============================] - 475s 3s/step - loss: 0.4648 - accuracy: 0.8385 - val_loss: 0.6666 - val_accuracy: 0.7534 - lr: 4.0000e-06\n","Epoch 16/100\n","183/183 [==============================] - 480s 3s/step - loss: 0.4492 - accuracy: 0.8391 - val_loss: 0.6774 - val_accuracy: 0.7380 - lr: 4.0000e-06\n","Epoch 17/100\n","183/183 [==============================] - 475s 3s/step - loss: 0.4571 - accuracy: 0.8384 - val_loss: 0.6613 - val_accuracy: 0.7466 - lr: 1.0000e-06\n","Epoch 18/100\n","183/183 [==============================] - 488s 3s/step - loss: 0.4544 - accuracy: 0.8394 - val_loss: 0.6662 - val_accuracy: 0.7449 - lr: 1.0000e-06\n","Epoch 19/100\n","183/183 [==============================] - 475s 3s/step - loss: 0.4558 - accuracy: 0.8379 - val_loss: 0.6639 - val_accuracy: 0.7534 - lr: 1.0000e-06\n"]}],"source":["# 모델 훈련\n","history = model.fit(\n","    train_generator,\n","    epochs=100,\n","    validation_data=validation_generator,\n","    callbacks=[reduce_lr, early_stopping],\n","    class_weight=class_weights\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"R0vRn2p3Z3xJ","outputId":"7c6fe0d4-cda2-4b13-db97-1659fdc28a5d"},"outputs":[{"name":"stdout","output_type":"stream","text":["37/37 [==============================] - 72s 2s/step - loss: 0.6713 - accuracy: 0.7432\n","Validation Loss: 0.6712750792503357\n","Validation Accuracy: 0.7431507110595703\n"]}],"source":["# 검증 데이터 확인\n","loss, accuracy = model.evaluate(validation_generator)\n","print(f\"Validation Loss: {loss}\")\n","print(f\"Validation Accuracy: {accuracy}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"sB5d2u6qsafF","outputId":"7bb75036-044d-49d4-9b25-11196e988327"},"outputs":[{"name":"stdout","output_type":"stream","text":["36/36 [==============================] - 911s 26s/step - loss: 0.6478 - accuracy: 0.7520\n","Test Loss: 0.6477816700935364\n","Test Accuracy: 0.751978874206543\n"]}],"source":["# 테스트 데이터 전체 검증\n","test_loss, test_accuracy = model.evaluate(test_generator)\n","print(f\"Test Loss: {test_loss}\")\n","print(f\"Test Accuracy: {test_accuracy}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"gu7AzcLndjh3"},"outputs":[],"source":["# 모델 저장\n","model.save('/content/drive/MyDrive/MobileNetV1_06.h5')"]},{"cell_type":"markdown","metadata":{"id":"p5kkjYL9WQnh"},"source":["#Sub"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J1E-s7AEBlW8"},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","from tensorflow.keras.preprocessing import image\n","\n","# 이미지 전처리 함수\n","def load_and_prepare_image(image_path, target_size=(224, 224)):\n","    img = image.load_img(image_path, target_size=target_size)\n","    img_tensor = image.img_to_array(img)\n","    img_tensor = np.expand_dims(img_tensor, axis=0)\n","    img_tensor /= 255.0  # 정규화\n","    return img_tensor\n","\n","# 감정을 예측하는 함수\n","def predict_emotion(model, img_path):\n","    print(\"이미지 로딩 중...\")\n","    test_image = load_and_prepare_image(img_path)\n","    print(\"감정 예측 중...\")\n","    prediction = model.predict(test_image)\n","    return prediction\n","\n","# 모델 경로와 이미지 경로\n","model_path = '/content/drive/MyDrive/MobileNetV1-6.h5'  # 학습된 모델 경로\n","image_path = '/content/drive/MyDrive/testimg/5.jpg'  # 테스트할 이미지 경로\n","\n","# 모델 로드\n","model = load_model(model_path)\n","\n","# 예측 결과 얻기\n","predictions = predict_emotion(model, image_path)\n","emotion_index = np.argmax(predictions)\n","emotion_labels = ['화남', '행복', '슬픔', '충격']  # 실제 감정 레이블로 변경\n","\n","print(\"예측된 감정:\", emotion_labels[emotion_index])\n","print(\"각 감정의 비율:\")\n","for i, label in enumerate(emotion_labels):\n","    print(f\"{label}: {predictions[0][i] * 100:.2f}%\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"Mo87OeFDBahU"},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import load_model\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import accuracy_score\n","\n","# 모델 파일 경로\n","model_path = '/content/drive/MyDrive/MobileNetV1_01.h5'\n","\n","# 모델 로드\n","model = load_model(model_path)\n","\n","# 테스트 데이터 디렉토리 경로\n","test_data_dir = '/content/drive/MyDrive/Data/img/test'\n","\n","# 테스트 데이터를 위한 ImageDataGenerator 생성\n","val_test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# 테스트 데이터 생성기 생성\n","test_generator = val_test_datagen.flow_from_directory(\n","    test_data_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical',\n","    shuffle=False\n",")\n","\n","# 테스트 데이터 전체 검증\n","test_loss, test_accuracy = model.evaluate(test_generator)\n","print(f\"Test Loss: {test_loss}\")\n","print(f\"Test Accuracy: {test_accuracy}\")\n","\n","# 예측 값 얻기\n","predictions = model.predict(test_generator)\n","predicted_classes = np.argmax(predictions, axis=1)\n","true_classes = test_generator.classes\n","class_labels = list(test_generator.class_indices.keys())\n","\n","# 각 클래스별 정확도 계산 및 잘못 예측한 경우 정리\n","misclassified_summary = []\n","\n","for i, label in enumerate(class_labels):\n","    indices = np.where(true_classes == i)[0]\n","    class_accuracy = accuracy_score(true_classes[indices], predicted_classes[indices])\n","    print(f\"Accuracy for class {label}: {class_accuracy * 100:.2f}%\")\n","\n","    # 잘못 예측한 경우 저장\n","    misclassified_indices = indices[true_classes[indices] != predicted_classes[indices]]\n","    for index in misclassified_indices:\n","        true_label = class_labels[true_classes[index]]\n","        predicted_label = class_labels[predicted_classes[index]]\n","        misclassified_summary.append((true_label, predicted_label))\n","\n","# DataFrame으로 정리\n","misclassified_df = pd.DataFrame(misclassified_summary, columns=[\"True Label\", \"Predicted Label\"])\n","\n","# 잘못 예측한 경우를 집계\n","misclassified_counts = misclassified_df.groupby(['True Label', 'Predicted Label']).size().reset_index(name='Count')\n","\n","# 정리된 결과 출력\n","print(\"\\nMisclassified Cases Summary:\")\n","print(misclassified_counts)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qnC2OS5xKw3S"},"outputs":[],"source":["from tensorflow.keras.models import load_model\n","from tensorflow.keras.preprocessing import image\n","\n","val_test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# 모델 파일 경로\n","model_path = '/content/drive/MyDrive/MobileNetV1_04.h5'\n","\n","# h5 모델 파일 로드\n","model = load_model(model_path)\n","\n","# 테스트 데이터 전체 검증\n","test_loss, test_accuracy = model.evaluate(test_generator)\n","print(f\"Test Loss: {test_loss}\")\n","print(f\"Test Accuracy: {test_accuracy}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":81323,"status":"ok","timestamp":1716251793669,"user":{"displayName":"박태우","userId":"12492517778931241346"},"user_tz":-540},"id":"rqu5puDVdzcq","outputId":"ef4aaa10-54e6-4bbf-c076-e0b156dafaf7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 1137 images belonging to 4 classes.\n","36/36 [==============================] - 77s 2s/step\n","Accuracy for class anger: 62.86%\n","Accuracy for class happy: 87.92%\n","Accuracy for class panic: 81.45%\n","Accuracy for class sadness: 66.55%\n","\n","Misclassified Cases Summary:\n","   True Label Predicted Label  Count\n","0       anger           happy     14\n","1       anger           panic     51\n","2       anger         sadness     39\n","3       happy           anger     22\n","4       happy           panic      2\n","5       happy         sadness     12\n","6       panic           anger     42\n","7       panic           happy      6\n","8       panic         sadness      3\n","9     sadness           anger     61\n","10    sadness           happy     11\n","11    sadness           panic     23\n"]}],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.models import load_model\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import accuracy_score\n","\n","# 모델 파일 경로\n","model_path = '/content/drive/MyDrive/MobileNetV1_04.h5'\n","\n","# 모델 로드\n","model = load_model(model_path)\n","\n","# 테스트 데이터 디렉토리 경로\n","test_data_dir = '/content/drive/MyDrive/Data/img/test'\n","\n","# 테스트 데이터를 위한 ImageDataGenerator 생성\n","val_test_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# 테스트 데이터 생성기 생성\n","test_generator = val_test_datagen.flow_from_directory(\n","    test_data_dir,\n","    target_size=(224, 224),\n","    batch_size=32,\n","    class_mode='categorical',\n","    shuffle=False\n",")\n","\n","# 예측 값 얻기\n","predictions = model.predict(test_generator)\n","predicted_classes = np.argmax(predictions, axis=1)\n","true_classes = test_generator.classes\n","class_labels = list(test_generator.class_indices.keys())\n","\n","# 각 클래스별 정확도 계산 및 잘못 예측한 경우 정리\n","misclassified_summary = []\n","\n","for i, label in enumerate(class_labels):\n","    indices = np.where(true_classes == i)[0]\n","    class_accuracy = accuracy_score(true_classes[indices], predicted_classes[indices])\n","    print(f\"Accuracy for class {label}: {class_accuracy * 100:.2f}%\")\n","\n","    # 잘못 예측한 경우 저장\n","    misclassified_indices = indices[true_classes[indices] != predicted_classes[indices]]\n","    for index in misclassified_indices:\n","        true_label = class_labels[true_classes[index]]\n","        predicted_label = class_labels[predicted_classes[index]]\n","        misclassified_summary.append((true_label, predicted_label))\n","\n","# DataFrame으로 정리\n","misclassified_df = pd.DataFrame(misclassified_summary, columns=[\"True Label\", \"Predicted Label\"])\n","\n","# 잘못 예측한 경우를 집계\n","misclassified_counts = misclassified_df.groupby(['True Label', 'Predicted Label']).size().reset_index(name='Count')\n","\n","# 정리된 결과 출력\n","print(\"\\nMisclassified Cases Summary:\")\n","print(misclassified_counts)\n"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}