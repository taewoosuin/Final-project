{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xV7z-yIHva2K",
        "outputId": "f088efe5-fdda-4e7f-d26c-c9c5d36e220c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECxNcYjjUaKH"
      },
      "source": [
        "#데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nI_ceMCZw0N"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dropout, Dense, Reshape, SeparableConv2D, Conv2D, BatchNormalization, Multiply, Layer,Attention, LayerNormalization, Add\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUgAMPfsSHrG"
      },
      "outputs": [],
      "source": [
        "def custom_augmentation(image):\n",
        "    image = random_erase(image, p=1, scale=(0.05, 0.05))\n",
        "    return image\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def random_erase(image, p=1, scale=(0.05, 0.05)):\n",
        "    if np.random.rand() > p:\n",
        "        return image\n",
        "\n",
        "    h, w, _ = image.shape\n",
        "    erase_area = np.random.uniform(scale[0], scale[1], 2)\n",
        "    erase_h = int(h * erase_area[0])\n",
        "    erase_w = int(w * erase_area[1])\n",
        "\n",
        "    erase_x = np.random.randint(0, w - erase_w + 1)\n",
        "    erase_y = np.random.randint(0, h - erase_h + 1)\n",
        "\n",
        "    image[erase_y:erase_y + erase_h, erase_x:erase_x + erase_w, :] = np.random.randint(0, 256, (erase_h, erase_w, 3), dtype=np.uint8)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0C-5LhNHJapI",
        "outputId": "b5aa9c5a-b7fb-4baa-8ed8-0fb096203955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5501 images belonging to 4 classes.\n",
            "Found 1093 images belonging to 4 classes.\n"
          ]
        }
      ],
      "source": [
        "# 데이터 경로\n",
        "train_data_dir = '/content/drive/MyDrive/Data/img_relabel/train'\n",
        "validation_data_dir = '/content/drive/MyDrive/Data/img_relabel/val'\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=(0.8, 1.0),\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest',\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    preprocessing_function=custom_augmentation\n",
        ")\n",
        "\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# 이미지 불러오기\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = val_test_datagen.flow_from_directory(\n",
        "    validation_data_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AyS03TtfQzB"
      },
      "source": [
        "#MobileNetV1 기반 모델링(실행환경 T4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBTsW46RIkF5"
      },
      "outputs": [],
      "source": [
        "# Base model 정의\n",
        "def create_base_model(input_shape):\n",
        "    # 베이스모델에서 가중치를 가져옴\n",
        "    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    base_model.trainable = False\n",
        "    # 모델의 일부를 버리면서 출력층 지정\n",
        "    layer_index = -14\n",
        "    output_layer = base_model.layers[layer_index].output\n",
        "    # 변형된 베이스 모델 생성\n",
        "    base_model = Model(inputs=base_model.input, outputs=output_layer)\n",
        "\n",
        "    return base_model\n",
        "\n",
        "# 패치 추출 레이어 정의\n",
        "def patch_extraction_layer():\n",
        "    return tf.keras.Sequential([\n",
        "        SeparableConv2D(256, kernel_size=4, strides=4, padding='same', activation='relu'),\n",
        "        SeparableConv2D(256, kernel_size=2, strides=2, padding='valid', activation='relu'),\n",
        "        Conv2D(256, kernel_size=1, strides=1, padding='valid', activation='relu')\n",
        "    ], name='patch_extraction')\n",
        "\n",
        "# Pre-classification 레이어 정의\n",
        "def create_pre_classification_layer():\n",
        "    return tf.keras.Sequential([\n",
        "        Dense(32, activation='relu'),\n",
        "        BatchNormalization()\n",
        "    ], name='pre_classification')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ba41_ozA4V70",
        "outputId": "73ddf69a-36e7-4fc0-d7ba-b15c46502f8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n",
            "17225924/17225924 [==============================] - 1s 0us/step\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " model (Functional)          (None, 14, 14, 512)          1627840   ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " patch_extraction (Sequenti  (None, 2, 2, 256)            272128    ['model[0][0]']               \n",
            " al)                                                                                              \n",
            "                                                                                                  \n",
            " gap (GlobalAveragePooling2  (None, 256)                  0         ['patch_extraction[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 256)                  0         ['gap[0][0]']                 \n",
            "                                                                                                  \n",
            " pre_classification (Sequen  (None, 32)                   8352      ['dropout[0][0]']             \n",
            " tial)                                                                                            \n",
            "                                                                                                  \n",
            " attention (Attention)       (None, 32)                   1         ['pre_classification[0][0]',  \n",
            "                                                                     'pre_classification[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 32)                   0         ['pre_classification[0][0]',  \n",
            "                                                                     'attention[0][0]']           \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 4)                    132       ['add[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1908453 (7.28 MB)\n",
            "Trainable params: 280549 (1.07 MB)\n",
            "Non-trainable params: 1627904 (6.21 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "def create_MobilePatchAttentionNet(input_shape, num_classes):\n",
        "    base_model = create_base_model(input_shape)\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # 기본 모델\n",
        "    x = base_model(inputs)\n",
        "    # 패치 추출 레이어\n",
        "    x = patch_extraction_layer()(x)\n",
        "    # GlobalAveragePooling2D 및 Dropout\n",
        "    x = GlobalAveragePooling2D(name='gap')(x)\n",
        "    x = Dropout(0.1)(x)\n",
        "    # 사전 분류 레이어\n",
        "    x = create_pre_classification_layer()(x)\n",
        "    # Self-Attention 추가\n",
        "    attn_output = Attention(use_scale=True)([x, x])\n",
        "    x = Add()([x, attn_output])\n",
        "    # 출력층\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# 모델 생성 및 컴파일\n",
        "model = create_MobilePatchAttentionNet(input_shape=(224, 224, 3), num_classes=4)\n",
        "model.compile(optimizer=Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 모델 요약 출력\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQdn9ZuV0CeI"
      },
      "outputs": [],
      "source": [
        "# 학습률 조정 콜백\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=2, min_delta=0.005, min_lr=1e-7)\n",
        "\n",
        "# EarlyStopping 콜백\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, min_delta=0.005, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9Qda9o00Ex4",
        "outputId": "f021ca35-f94c-46e8-b8f5-0a0cb7c90cba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "172/172 [==============================] - 3774s 22s/step - loss: 1.3798 - accuracy: 0.2923 - val_loss: 1.3982 - val_accuracy: 0.2644 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "172/172 [==============================] - 408s 2s/step - loss: 1.2456 - accuracy: 0.4290 - val_loss: 1.3477 - val_accuracy: 0.2415 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "172/172 [==============================] - 404s 2s/step - loss: 1.0951 - accuracy: 0.5239 - val_loss: 1.2314 - val_accuracy: 0.4419 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "172/172 [==============================] - 410s 2s/step - loss: 1.0138 - accuracy: 0.5710 - val_loss: 1.2969 - val_accuracy: 0.4511 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "172/172 [==============================] - 421s 2s/step - loss: 0.9721 - accuracy: 0.5943 - val_loss: 1.0516 - val_accuracy: 0.5169 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "172/172 [==============================] - 418s 2s/step - loss: 0.9454 - accuracy: 0.6068 - val_loss: 1.3130 - val_accuracy: 0.4684 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "172/172 [==============================] - 412s 2s/step - loss: 0.9263 - accuracy: 0.6192 - val_loss: 1.0381 - val_accuracy: 0.5608 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "172/172 [==============================] - 407s 2s/step - loss: 0.8984 - accuracy: 0.6313 - val_loss: 1.0704 - val_accuracy: 0.5682 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "172/172 [==============================] - 405s 2s/step - loss: 0.8933 - accuracy: 0.6297 - val_loss: 1.5234 - val_accuracy: 0.4675 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "172/172 [==============================] - 407s 2s/step - loss: 0.8747 - accuracy: 0.6484 - val_loss: 1.2694 - val_accuracy: 0.5242 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "172/172 [==============================] - 408s 2s/step - loss: 0.8094 - accuracy: 0.6802 - val_loss: 0.9531 - val_accuracy: 0.5892 - lr: 1.0000e-04\n",
            "Epoch 12/100\n",
            "172/172 [==============================] - 406s 2s/step - loss: 0.7964 - accuracy: 0.6831 - val_loss: 0.9498 - val_accuracy: 0.5993 - lr: 1.0000e-04\n",
            "Epoch 13/100\n",
            "172/172 [==============================] - 403s 2s/step - loss: 0.7877 - accuracy: 0.6830 - val_loss: 0.9449 - val_accuracy: 0.6029 - lr: 1.0000e-04\n",
            "Epoch 14/100\n",
            "172/172 [==============================] - 396s 2s/step - loss: 0.7772 - accuracy: 0.6895 - val_loss: 0.9298 - val_accuracy: 0.6048 - lr: 1.0000e-04\n",
            "Epoch 15/100\n",
            "172/172 [==============================] - 406s 2s/step - loss: 0.7888 - accuracy: 0.6786 - val_loss: 0.9283 - val_accuracy: 0.6038 - lr: 1.0000e-04\n",
            "Epoch 16/100\n",
            "172/172 [==============================] - 398s 2s/step - loss: 0.7841 - accuracy: 0.6835 - val_loss: 0.9322 - val_accuracy: 0.6148 - lr: 1.0000e-04\n",
            "Epoch 17/100\n",
            "172/172 [==============================] - 404s 2s/step - loss: 0.7714 - accuracy: 0.6981 - val_loss: 0.9867 - val_accuracy: 0.6038 - lr: 1.0000e-04\n",
            "Epoch 18/100\n",
            "172/172 [==============================] - 394s 2s/step - loss: 0.7505 - accuracy: 0.6995 - val_loss: 0.9559 - val_accuracy: 0.6038 - lr: 1.0000e-04\n",
            "Epoch 19/100\n",
            "172/172 [==============================] - 399s 2s/step - loss: 0.7657 - accuracy: 0.6970 - val_loss: 0.9303 - val_accuracy: 0.6038 - lr: 1.0000e-05\n",
            "Epoch 20/100\n",
            "172/172 [==============================] - 405s 2s/step - loss: 0.7637 - accuracy: 0.6941 - val_loss: 0.9215 - val_accuracy: 0.6130 - lr: 1.0000e-05\n",
            "Epoch 21/100\n",
            "172/172 [==============================] - 406s 2s/step - loss: 0.7758 - accuracy: 0.6904 - val_loss: 0.9216 - val_accuracy: 0.6102 - lr: 1.0000e-06\n"
          ]
        }
      ],
      "source": [
        "# 모델 훈련\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=100,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=[reduce_lr, early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IYiJIHEuMmcM",
        "outputId": "25b5a4e5-0dc0-44c6-adf8-3eab3856775e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " model (Functional)          (None, 14, 14, 512)          1627840   ['input_2[0][0]']             \n",
            "                                                                                                  \n",
            " patch_extraction (Sequenti  (None, None, None, 256)      272128    ['model[0][0]']               \n",
            " al)                                                                                              \n",
            "                                                                                                  \n",
            " gap (GlobalAveragePooling2  (None, 256)                  0         ['patch_extraction[0][0]']    \n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 256)                  0         ['gap[0][0]']                 \n",
            "                                                                                                  \n",
            " pre_classification (Sequen  (None, 32)                   8352      ['dropout[0][0]']             \n",
            " tial)                                                                                            \n",
            "                                                                                                  \n",
            " attention (Attention)       (None, 32)                   1         ['pre_classification[0][0]',  \n",
            "                                                                     'pre_classification[0][0]']  \n",
            "                                                                                                  \n",
            " add (Add)                   (None, 32)                   0         ['pre_classification[0][0]',  \n",
            "                                                                     'attention[0][0]']           \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 4)                    132       ['add[0][0]']                 \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1908453 (7.28 MB)\n",
            "Trainable params: 1614309 (6.16 MB)\n",
            "Non-trainable params: 294144 (1.12 MB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#파인튜닝\n",
        "#베이스 모델 학습 가능 레이어 지정\n",
        "base_model = model.layers[1]\n",
        "base_model.trainable = True\n",
        "num_freeze_layers = 43\n",
        "\n",
        "# 상위 레이어 학습 가능/하위 레이어는 고정\n",
        "for layer in base_model.layers[:num_freeze_layers]:\n",
        "    layer.trainable = False\n",
        "for layer in base_model.layers[num_freeze_layers:]:\n",
        "    if isinstance(layer, BatchNormalization):\n",
        "        layer.trainable = False\n",
        "    else:\n",
        "        layer.trainable = True\n",
        "\n",
        "# 모델 재컴파일\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 모델 요약 출력\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qG5UtjAI11_K",
        "outputId": "c13ed53c-a4ba-4f05-a66a-30db6e14748f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "172/172 [==============================] - 414s 2s/step - loss: 0.8184 - accuracy: 0.6742 - val_loss: 1.0923 - val_accuracy: 0.5691 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            "172/172 [==============================] - 423s 2s/step - loss: 0.6705 - accuracy: 0.7328 - val_loss: 1.3328 - val_accuracy: 0.5938 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            "172/172 [==============================] - 423s 2s/step - loss: 0.6140 - accuracy: 0.7593 - val_loss: 1.0289 - val_accuracy: 0.6661 - lr: 1.0000e-04\n",
            "Epoch 4/100\n",
            "172/172 [==============================] - 425s 2s/step - loss: 0.5706 - accuracy: 0.7817 - val_loss: 0.9649 - val_accuracy: 0.6011 - lr: 1.0000e-04\n",
            "Epoch 5/100\n",
            "172/172 [==============================] - 433s 3s/step - loss: 0.5348 - accuracy: 0.7973 - val_loss: 1.0929 - val_accuracy: 0.6514 - lr: 1.0000e-04\n",
            "Epoch 6/100\n",
            "172/172 [==============================] - 426s 2s/step - loss: 0.4497 - accuracy: 0.8358 - val_loss: 0.6857 - val_accuracy: 0.7484 - lr: 1.0000e-05\n",
            "Epoch 7/100\n",
            "172/172 [==============================] - 425s 2s/step - loss: 0.4409 - accuracy: 0.8373 - val_loss: 0.6273 - val_accuracy: 0.7557 - lr: 1.0000e-05\n",
            "Epoch 8/100\n",
            "172/172 [==============================] - 431s 3s/step - loss: 0.4309 - accuracy: 0.8477 - val_loss: 0.6334 - val_accuracy: 0.7493 - lr: 1.0000e-05\n",
            "Epoch 9/100\n",
            "172/172 [==============================] - 422s 2s/step - loss: 0.4081 - accuracy: 0.8520 - val_loss: 0.6412 - val_accuracy: 0.7521 - lr: 1.0000e-05\n",
            "Epoch 10/100\n",
            "172/172 [==============================] - 421s 2s/step - loss: 0.3935 - accuracy: 0.8591 - val_loss: 0.6245 - val_accuracy: 0.7530 - lr: 1.0000e-06\n",
            "Epoch 11/100\n",
            "172/172 [==============================] - 417s 2s/step - loss: 0.3962 - accuracy: 0.8578 - val_loss: 0.6448 - val_accuracy: 0.7521 - lr: 1.0000e-06\n",
            "Epoch 12/100\n",
            "172/172 [==============================] - 411s 2s/step - loss: 0.3979 - accuracy: 0.8549 - val_loss: 0.6292 - val_accuracy: 0.7548 - lr: 1.0000e-07\n"
          ]
        }
      ],
      "source": [
        "# 모델 훈련\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=100,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=[reduce_lr, early_stopping]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ljnV7EoWv8Ak",
        "outputId": "dcba0ce1-8dab-4517-abbb-824834f2aea5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "35/35 [==============================] - 60s 2s/step - loss: 0.6272 - accuracy: 0.7539\n",
            "Validation Loss: 0.6271569728851318\n",
            "Validation Accuracy: 0.7538883686065674\n"
          ]
        }
      ],
      "source": [
        "# 검증 데이터 확인\n",
        "loss, accuracy = model.evaluate(validation_generator)\n",
        "print(f\"Validation Loss: {loss}\")\n",
        "print(f\"Validation Accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "collapsed": true,
        "id": "gu7AzcLndjh3",
        "outputId": "e4b4e3cd-29d2-4b2b-8585-af0eb023f872"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# 모델 저장\n",
        "model.save('/content/drive/MyDrive/MPANET-01.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3pj5Tf_G0IkG",
        "outputId": "92cbd629-d4af-4a01-c9bd-66b4507f9737"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1137 images belonging to 4 classes.\n",
            "36/36 [==============================] - 664s 19s/step - loss: 0.3761 - accuracy: 0.8672\n",
            "Test Loss: 0.376079797744751\n",
            "Test Accuracy: 0.8671943545341492\n",
            "36/36 [==============================] - 61s 2s/step\n",
            "Accuracy for class anger: 66.43%\n",
            "Accuracy for class happy: 96.31%\n",
            "Accuracy for class panic: 95.64%\n",
            "Accuracy for class sadness: 88.03%\n",
            "\n",
            "Misclassified Cases Summary:\n",
            "   True Label Predicted Label  Count\n",
            "0       anger           happy      9\n",
            "1       anger           panic     46\n",
            "2       anger         sadness     39\n",
            "3       happy           anger      7\n",
            "4       happy           panic      1\n",
            "5       happy         sadness      3\n",
            "6       panic           anger      8\n",
            "7       panic           happy      2\n",
            "8       panic         sadness      2\n",
            "9     sadness           anger     15\n",
            "10    sadness           happy      5\n",
            "11    sadness           panic     14\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# 모델 파일 경로\n",
        "model_path = '/content/drive/MyDrive/MPANET-01.h5'\n",
        "\n",
        "# 모델 로드\n",
        "model = load_model(model_path)\n",
        "\n",
        "# 테스트 데이터 디렉토리 경로\n",
        "test_data_dir = '/content/drive/MyDrive/Data/img/test'\n",
        "\n",
        "# 테스트 데이터를 위한 ImageDataGenerator 생성\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# 테스트 데이터 생성기 생성\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# 테스트 데이터 전체 검증\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# 예측 값 얻기\n",
        "predictions = model.predict(test_generator)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "true_classes = test_generator.classes\n",
        "class_labels = list(test_generator.class_indices.keys())\n",
        "\n",
        "# 각 클래스별 정확도 계산 및 잘못 예측한 경우 정리\n",
        "misclassified_summary = []\n",
        "\n",
        "for i, label in enumerate(class_labels):\n",
        "    indices = np.where(true_classes == i)[0]\n",
        "    class_accuracy = accuracy_score(true_classes[indices], predicted_classes[indices])\n",
        "    print(f\"Accuracy for class {label}: {class_accuracy * 100:.2f}%\")\n",
        "\n",
        "    # 잘못 예측한 경우 저장\n",
        "    misclassified_indices = indices[true_classes[indices] != predicted_classes[indices]]\n",
        "    for index in misclassified_indices:\n",
        "        true_label = class_labels[true_classes[index]]\n",
        "        predicted_label = class_labels[predicted_classes[index]]\n",
        "        misclassified_summary.append((true_label, predicted_label))\n",
        "\n",
        "# DataFrame으로 정리\n",
        "misclassified_df = pd.DataFrame(misclassified_summary, columns=[\"True Label\", \"Predicted Label\"])\n",
        "\n",
        "# 잘못 예측한 경우를 집계\n",
        "misclassified_counts = misclassified_df.groupby(['True Label', 'Predicted Label']).size().reset_index(name='Count')\n",
        "\n",
        "# 정리된 결과 출력\n",
        "print(\"\\nMisclassified Cases Summary:\")\n",
        "print(misclassified_counts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5kkjYL9WQnh"
      },
      "source": [
        "#Sub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "J1E-s7AEBlW8",
        "outputId": "67d6d001-9e39-4b7a-cbe8-42fb6d1bbc25"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "No file or directory found at /content/drive/MyDrive/MobileNetV1-6.h5",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-d891ecc014a2>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# 모델 로드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# 예측 결과 얻기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                             raise IOError(\n\u001b[0m\u001b[1;32m    235\u001b[0m                                 \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                             )\n",
            "\u001b[0;31mOSError\u001b[0m: No file or directory found at /content/drive/MyDrive/MobileNetV1-6.h5"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "\n",
        "# 이미지 전처리 함수\n",
        "def load_and_prepare_image(image_path, target_size=(224, 224)):\n",
        "    img = image.load_img(image_path, target_size=target_size)\n",
        "    img_tensor = image.img_to_array(img)\n",
        "    img_tensor = np.expand_dims(img_tensor, axis=0)\n",
        "    img_tensor /= 255.0  # 정규화\n",
        "    return img_tensor\n",
        "\n",
        "# 감정을 예측하는 함수\n",
        "def predict_emotion(model, img_path):\n",
        "    print(\"이미지 로딩 중...\")\n",
        "    test_image = load_and_prepare_image(img_path)\n",
        "    print(\"감정 예측 중...\")\n",
        "    prediction = model.predict(test_image)\n",
        "    return prediction\n",
        "\n",
        "# 모델 경로와 이미지 경로\n",
        "model_path = '/content/drive/MyDrive/MPANET-01.h5'  # 학습된 모델 경로\n",
        "image_path = '/content/drive/MyDrive/testimg/5.jpg'  # 테스트할 이미지 경로\n",
        "\n",
        "# 모델 로드\n",
        "model = load_model(model_path)\n",
        "\n",
        "# 예측 결과 얻기\n",
        "predictions = predict_emotion(model, image_path)\n",
        "emotion_index = np.argmax(predictions)\n",
        "emotion_labels = ['화남', '행복', '슬픔', '충격']  # 실제 감정 레이블로 변경\n",
        "\n",
        "print(\"예측된 감정:\", emotion_labels[emotion_index])\n",
        "print(\"각 감정의 비율:\")\n",
        "for i, label in enumerate(emotion_labels):\n",
        "    print(f\"{label}: {predictions[0][i] * 100:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}