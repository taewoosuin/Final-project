{"cells":[{"cell_type":"code","execution_count":3,"id":"ug76HbXyq21X","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15455,"status":"ok","timestamp":1717376936741,"user":{"displayName":"estcampus.ai.9","userId":"07573217422646033083"},"user_tz":-540},"id":"ug76HbXyq21X","outputId":"6e877700-21b4-40b1-e5af-f1b2e346805d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"69c706c7-ffc0-487c-8250-7d2b049719f7","metadata":{"collapsed":true,"id":"69c706c7-ffc0-487c-8250-7d2b049719f7","scrolled":true},"outputs":[],"source":["import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, LeakyReLU, BatchNormalization, Embedding, Input, Concatenate\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","import matplotlib.pyplot as plt\n","\n","# 데이터 불러오기\n","data_dir = '/content/drive/MyDrive/code/gan'\n","categories = ['angry', 'happy', 'panic', 'sadness']\n","\n","# 이미지 로드 함수\n","def load_images(data_dir, categories):\n","    images = []\n","    labels = []\n","    for idx, category in enumerate(categories):\n","        category_dir = os.path.join(data_dir, category)\n","        for file in os.listdir(category_dir):\n","            if file.endswith('.png') or file.endswith('.jpg'):\n","                img_path = os.path.join(category_dir, file)\n","                img = load_img(img_path, target_size=(64, 64))\n","                img_array = img_to_array(img)\n","                images.append(img_array)\n","                labels.append(idx)\n","    images = np.array(images)\n","    labels = np.array(labels)\n","    return images, labels\n","\n","images, labels = load_images(data_dir, categories)\n","images = (images - 127.5) / 127.5  # Normalize images to [-1, 1]\n","\n","# GAN 모델 정의\n","def build_generator():\n","    noise = Input(shape=(100,))\n","    label = Input(shape=(1,), dtype='int32')\n","\n","    label_embedding = Embedding(len(categories), 100)(label)\n","    label_embedding = Flatten()(label_embedding)\n","\n","    model_input = Concatenate()([noise, label_embedding])\n","\n","    x = Dense(256 * 8 * 8, activation=\"relu\")(model_input)\n","    x = Reshape((8, 8, 256))(x)\n","    x = BatchNormalization(momentum=0.8)(x)\n","    x = Conv2DTranspose(128, kernel_size=4, strides=2, padding='same')(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = BatchNormalization(momentum=0.8)(x)\n","    x = Conv2DTranspose(64, kernel_size=4, strides=2, padding='same')(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = BatchNormalization(momentum=0.8)(x)\n","    img = Conv2DTranspose(3, kernel_size=4, strides=2, padding='same', activation='tanh')(x)\n","\n","    model = Model([noise, label], img)\n","    return model\n","\n","def build_discriminator():\n","    img = Input(shape=(64, 64, 3))\n","    label = Input(shape=(1,), dtype='int32')\n","\n","    label_embedding = Embedding(len(categories), np.prod((64, 64, 3)))(label)\n","    label_embedding = Flatten()(label_embedding)\n","    label_embedding = Reshape((64, 64, 3))(label_embedding)\n","\n","    concatenated = Concatenate()([img, label_embedding])\n","\n","    x = Conv2D(64, kernel_size=4, strides=2, padding='same')(concatenated)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = Conv2D(128, kernel_size=4, strides=2, padding='same')(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = Flatten()(x)\n","    x = Dense(1, activation='sigmoid')(x)\n","\n","    model = Model([img, label], x)\n","    return model\n","\n","# Compile the discriminator\n","discriminator = build_discriminator()\n","discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n","\n","# Build and compile the combined model\n","generator = build_generator()\n","noise = Input(shape=(100,))\n","label = Input(shape=(1,))\n","img = generator([noise, label])\n","discriminator.trainable = False\n","validity = discriminator([img, label])\n","combined = Model([noise, label], validity)\n","combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n","\n","# GAN 학습\n","def train(epochs, batch_size=128, save_interval=50):\n","    half_batch = int(batch_size / 2)\n","\n","    for epoch in range(epochs):\n","        idx = np.random.randint(0, images.shape[0], half_batch)\n","        imgs, labels_batch = images[idx], labels[idx].reshape(-1, 1)\n","\n","        noise = np.random.normal(0, 1, (half_batch, 100))\n","        gen_labels = np.random.randint(0, len(categories), half_batch).reshape(-1, 1)\n","        gen_imgs = generator.predict([noise, gen_labels])\n","\n","        d_loss_real = discriminator.train_on_batch([imgs, labels_batch], np.ones((half_batch, 1)))\n","        d_loss_fake = discriminator.train_on_batch([gen_imgs, gen_labels], np.zeros((half_batch, 1)))\n","        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","\n","        noise = np.random.normal(0, 1, (batch_size, 100))\n","        sampled_labels = np.random.randint(0, len(categories), batch_size).reshape(-1, 1)\n","        valid_y = np.array([1] * batch_size)\n","\n","        g_loss = combined.train_on_batch([noise, sampled_labels], valid_y)\n","\n","        print(f\"{epoch} [D loss: {d_loss[0]} | D accuracy: {d_loss[1]}] [G loss: {g_loss}]\")\n","\n","        if epoch % save_interval == 0:\n","            save_imgs(epoch)\n","\n","def save_imgs(epoch):\n","    r, c = 2, 2\n","    noise = np.random.normal(0, 1, (r * c, 100))\n","    sampled_labels = np.array([num for num in range(r * c)]).reshape(-1, 1)\n","    gen_imgs = generator.predict([noise, sampled_labels])\n","    gen_imgs = 0.5 * gen_imgs + 0.5\n","\n","    fig, axs = plt.subplots(r, c)\n","    cnt = 0\n","    for i in range(r):\n","        for j in range(c):\n","            axs[i, j].imshow(gen_imgs[cnt, :, :, :])\n","            axs[i, j].set_title(categories[sampled_labels[cnt][0]])\n","            axs[i, j].axis('off')\n","            cnt += 1\n","    if not os.path.exists(\"gan_images\"):\n","        os.makedirs(\"gan_images\")\n","    fig.savefig(f\"gan_images/gan_image_{epoch}.png\")\n","    plt.close()\n","\n","train(epochs=10000, batch_size=32, save_interval=200)\n","\n","# 모델 저장\n","generator.save('/content/drive/MyDrive/code/gan_generator.h5')\n","discriminator.save('/content/drive/MyDrive/code/gan_discriminator.h5')\n","combined.save('/content/drive/MyDrive/code/gan_combined.h5')\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":5}
